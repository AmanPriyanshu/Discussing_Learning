{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Discussing the Perception of Perceptrons.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EE3vev9mQpX",
        "colab_type": "text"
      },
      "source": [
        "# Introduction:\n",
        "\n",
        "#### What are Perceptrons:\n",
        "In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.\n",
        "\n",
        "Small, fast and compact these Perceptrons are able to learn and develop an understanding for so many Datasets. Yes, they may not be as efficient an accurate as Deep Neural Networks but they are still extremely useful models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWIj4g52nkYh",
        "colab_type": "text"
      },
      "source": [
        "Let us begin by understanding what perceptrons really are by creating and deploying on for a simple dataset where everything greater than 0.5 can be labelled as a 1 and everything below can be labelled as 0. \n",
        "\n",
        "Before beginning Perceptrons though let us look at an even simpler model called the \"Linear Regression Model\" I guess unless there is a name like Perceptron for it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo6tsygToOQP",
        "colab_type": "text"
      },
      "source": [
        "### Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-kPBQ94l-1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h5yV4s-oVG9",
        "colab_type": "text"
      },
      "source": [
        "### Creating A Dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C-2_C_NyEww",
        "colab_type": "text"
      },
      "source": [
        "Creating a Dataset around y = mx + c where both are defined below I will also create some noise and variation so that might not be the optimal answer finally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRgt1D3HoUeH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3780605c-7266-4d87-ea4a-5b2f71796803"
      },
      "source": [
        "m = 121.7094\n",
        "c = 891.2648\n",
        "\n",
        "x = np.array([i for i in range(0, 50000, 3)][:16000])\n",
        "y = np.array([m*i+c+random.randint(-2, 2)*random.random() for i in range(0, 50000, 3)][:16000])\n",
        "print(y.shape, x.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16000,) (16000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYgTGf0FzZjL",
        "colab_type": "text"
      },
      "source": [
        "### Shuffling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scPLizzboTpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = np.array(range(y.shape[0]))\n",
        "np.random.shuffle(index)\n",
        "\n",
        "x = x[index]\n",
        "y = y[index]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCTUkI8T0Rjt",
        "colab_type": "text"
      },
      "source": [
        "### Model and Fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KisZFf3qzmmM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a14ab19-af90-4ae7-c383-7af7af0af7db"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='linear')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "history = model.fit(x, y, epochs=200, verbose=1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "500/500 [==============================] - 0s 951us/step - loss: 11031529127936.0000 - mae: 2876575.7500\n",
            "Epoch 2/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 10940790603776.0000 - mae: 2864707.2500\n",
            "Epoch 3/200\n",
            "500/500 [==============================] - 0s 980us/step - loss: 10850726313984.0000 - mae: 2852901.0000\n",
            "Epoch 4/200\n",
            "500/500 [==============================] - 0s 963us/step - loss: 10761100328960.0000 - mae: 2841085.2500\n",
            "Epoch 5/200\n",
            "500/500 [==============================] - 0s 900us/step - loss: 10671992340480.0000 - mae: 2829305.5000\n",
            "Epoch 6/200\n",
            "500/500 [==============================] - 0s 958us/step - loss: 10583229333504.0000 - mae: 2817522.7500\n",
            "Epoch 7/200\n",
            "500/500 [==============================] - 0s 924us/step - loss: 10494924554240.0000 - mae: 2805752.0000\n",
            "Epoch 8/200\n",
            "500/500 [==============================] - 0s 938us/step - loss: 10407058079744.0000 - mae: 2793973.0000\n",
            "Epoch 9/200\n",
            "500/500 [==============================] - 0s 933us/step - loss: 10319668707328.0000 - mae: 2782212.0000\n",
            "Epoch 10/200\n",
            "500/500 [==============================] - 0s 931us/step - loss: 10232709251072.0000 - mae: 2770453.7500\n",
            "Epoch 11/200\n",
            "500/500 [==============================] - 0s 919us/step - loss: 10146071707648.0000 - mae: 2758715.7500\n",
            "Epoch 12/200\n",
            "500/500 [==============================] - 0s 969us/step - loss: 10059630247936.0000 - mae: 2746955.2500\n",
            "Epoch 13/200\n",
            "500/500 [==============================] - 0s 906us/step - loss: 9973728804864.0000 - mae: 2735173.5000\n",
            "Epoch 14/200\n",
            "500/500 [==============================] - 0s 947us/step - loss: 9888188071936.0000 - mae: 2723439.0000\n",
            "Epoch 15/200\n",
            "500/500 [==============================] - 0s 971us/step - loss: 9803031117824.0000 - mae: 2711673.0000\n",
            "Epoch 16/200\n",
            "500/500 [==============================] - 0s 937us/step - loss: 9718343925760.0000 - mae: 2699929.0000\n",
            "Epoch 17/200\n",
            "500/500 [==============================] - 0s 943us/step - loss: 9633908391936.0000 - mae: 2688192.5000\n",
            "Epoch 18/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 9549725564928.0000 - mae: 2676417.7500\n",
            "Epoch 19/200\n",
            "500/500 [==============================] - 0s 952us/step - loss: 9466004111360.0000 - mae: 2664657.2500\n",
            "Epoch 20/200\n",
            "500/500 [==============================] - 0s 894us/step - loss: 9382574161920.0000 - mae: 2652916.5000\n",
            "Epoch 21/200\n",
            "500/500 [==============================] - 0s 900us/step - loss: 9299654868992.0000 - mae: 2641142.5000\n",
            "Epoch 22/200\n",
            "500/500 [==============================] - 0s 903us/step - loss: 9217187512320.0000 - mae: 2629417.5000\n",
            "Epoch 23/200\n",
            "500/500 [==============================] - 0s 954us/step - loss: 9134990688256.0000 - mae: 2617670.2500\n",
            "Epoch 24/200\n",
            "500/500 [==============================] - 0s 868us/step - loss: 9053172400128.0000 - mae: 2605922.7500\n",
            "Epoch 25/200\n",
            "500/500 [==============================] - 0s 931us/step - loss: 8971771445248.0000 - mae: 2594176.7500\n",
            "Epoch 26/200\n",
            "500/500 [==============================] - 0s 893us/step - loss: 8890730151936.0000 - mae: 2582434.7500\n",
            "Epoch 27/200\n",
            "500/500 [==============================] - 0s 988us/step - loss: 8809989799936.0000 - mae: 2570704.5000\n",
            "Epoch 28/200\n",
            "500/500 [==============================] - 0s 923us/step - loss: 8729649479680.0000 - mae: 2558939.0000\n",
            "Epoch 29/200\n",
            "500/500 [==============================] - 0s 930us/step - loss: 8649656238080.0000 - mae: 2547198.0000\n",
            "Epoch 30/200\n",
            "500/500 [==============================] - 0s 904us/step - loss: 8570075611136.0000 - mae: 2535439.7500\n",
            "Epoch 31/200\n",
            "500/500 [==============================] - 0s 927us/step - loss: 8490875617280.0000 - mae: 2523692.2500\n",
            "Epoch 32/200\n",
            "500/500 [==============================] - 0s 950us/step - loss: 8412073558016.0000 - mae: 2511963.7500\n",
            "Epoch 33/200\n",
            "500/500 [==============================] - 0s 893us/step - loss: 8333561430016.0000 - mae: 2500210.2500\n",
            "Epoch 34/200\n",
            "500/500 [==============================] - 0s 927us/step - loss: 8255419973632.0000 - mae: 2488468.7500\n",
            "Epoch 35/200\n",
            "500/500 [==============================] - 0s 960us/step - loss: 8177593614336.0000 - mae: 2476719.0000\n",
            "Epoch 36/200\n",
            "500/500 [==============================] - 0s 959us/step - loss: 8100208705536.0000 - mae: 2464961.7500\n",
            "Epoch 37/200\n",
            "500/500 [==============================] - 0s 954us/step - loss: 8023275208704.0000 - mae: 2453223.7500\n",
            "Epoch 38/200\n",
            "500/500 [==============================] - 0s 957us/step - loss: 7946658906112.0000 - mae: 2441484.0000\n",
            "Epoch 39/200\n",
            "500/500 [==============================] - 0s 905us/step - loss: 7870446829568.0000 - mae: 2429729.0000\n",
            "Epoch 40/200\n",
            "500/500 [==============================] - 0s 930us/step - loss: 7794584453120.0000 - mae: 2418019.7500\n",
            "Epoch 41/200\n",
            "500/500 [==============================] - 0s 926us/step - loss: 7719054999552.0000 - mae: 2406270.5000\n",
            "Epoch 42/200\n",
            "500/500 [==============================] - 0s 969us/step - loss: 7643915091968.0000 - mae: 2394529.5000\n",
            "Epoch 43/200\n",
            "500/500 [==============================] - 0s 953us/step - loss: 7569164730368.0000 - mae: 2382804.0000\n",
            "Epoch 44/200\n",
            "500/500 [==============================] - 0s 936us/step - loss: 7494771408896.0000 - mae: 2371049.2500\n",
            "Epoch 45/200\n",
            "500/500 [==============================] - 0s 929us/step - loss: 7420780740608.0000 - mae: 2359314.7500\n",
            "Epoch 46/200\n",
            "500/500 [==============================] - 0s 935us/step - loss: 7347178045440.0000 - mae: 2347593.2500\n",
            "Epoch 47/200\n",
            "500/500 [==============================] - 0s 947us/step - loss: 7273865281536.0000 - mae: 2335855.5000\n",
            "Epoch 48/200\n",
            "500/500 [==============================] - 0s 988us/step - loss: 7200920567808.0000 - mae: 2324114.5000\n",
            "Epoch 49/200\n",
            "500/500 [==============================] - 0s 944us/step - loss: 7128378507264.0000 - mae: 2312375.7500\n",
            "Epoch 50/200\n",
            "500/500 [==============================] - 0s 919us/step - loss: 7056210264064.0000 - mae: 2300641.7500\n",
            "Epoch 51/200\n",
            "500/500 [==============================] - 0s 936us/step - loss: 6984299446272.0000 - mae: 2288909.2500\n",
            "Epoch 52/200\n",
            "500/500 [==============================] - 0s 896us/step - loss: 6912789184512.0000 - mae: 2277156.0000\n",
            "Epoch 53/200\n",
            "500/500 [==============================] - 0s 931us/step - loss: 6841751306240.0000 - mae: 2265424.5000\n",
            "Epoch 54/200\n",
            "500/500 [==============================] - 0s 950us/step - loss: 6771043205120.0000 - mae: 2253690.2500\n",
            "Epoch 55/200\n",
            "500/500 [==============================] - 0s 977us/step - loss: 6700803293184.0000 - mae: 2241945.5000\n",
            "Epoch 56/200\n",
            "500/500 [==============================] - 0s 961us/step - loss: 6630921994240.0000 - mae: 2230242.5000\n",
            "Epoch 57/200\n",
            "500/500 [==============================] - 0s 951us/step - loss: 6561383055360.0000 - mae: 2218512.7500\n",
            "Epoch 58/200\n",
            "500/500 [==============================] - 0s 889us/step - loss: 6492165505024.0000 - mae: 2206795.5000\n",
            "Epoch 59/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 6423293460480.0000 - mae: 2195054.0000\n",
            "Epoch 60/200\n",
            "500/500 [==============================] - 0s 949us/step - loss: 6354818301952.0000 - mae: 2183324.5000\n",
            "Epoch 61/200\n",
            "500/500 [==============================] - 0s 930us/step - loss: 6286790361088.0000 - mae: 2171593.2500\n",
            "Epoch 62/200\n",
            "500/500 [==============================] - 0s 907us/step - loss: 6219056021504.0000 - mae: 2159881.2500\n",
            "Epoch 63/200\n",
            "500/500 [==============================] - 0s 957us/step - loss: 6151696547840.0000 - mae: 2148149.7500\n",
            "Epoch 64/200\n",
            "500/500 [==============================] - 0s 893us/step - loss: 6084725571584.0000 - mae: 2136430.5000\n",
            "Epoch 65/200\n",
            "500/500 [==============================] - 0s 904us/step - loss: 6018099576832.0000 - mae: 2124703.2500\n",
            "Epoch 66/200\n",
            "500/500 [==============================] - 0s 898us/step - loss: 5951783960576.0000 - mae: 2112981.5000\n",
            "Epoch 67/200\n",
            "500/500 [==============================] - 0s 897us/step - loss: 5885930242048.0000 - mae: 2101223.7500\n",
            "Epoch 68/200\n",
            "500/500 [==============================] - 0s 976us/step - loss: 5820479700992.0000 - mae: 2089526.2500\n",
            "Epoch 69/200\n",
            "500/500 [==============================] - 0s 917us/step - loss: 5755412938752.0000 - mae: 2077790.5000\n",
            "Epoch 70/200\n",
            "500/500 [==============================] - 0s 927us/step - loss: 5690694303744.0000 - mae: 2066100.1250\n",
            "Epoch 71/200\n",
            "500/500 [==============================] - 0s 917us/step - loss: 5626241482752.0000 - mae: 2054369.3750\n",
            "Epoch 72/200\n",
            "500/500 [==============================] - 0s 939us/step - loss: 5562175062016.0000 - mae: 2042639.2500\n",
            "Epoch 73/200\n",
            "500/500 [==============================] - 0s 920us/step - loss: 5498542751744.0000 - mae: 2030915.8750\n",
            "Epoch 74/200\n",
            "500/500 [==============================] - 0s 939us/step - loss: 5435348746240.0000 - mae: 2019193.6250\n",
            "Epoch 75/200\n",
            "500/500 [==============================] - 0s 919us/step - loss: 5372467216384.0000 - mae: 2007499.8750\n",
            "Epoch 76/200\n",
            "500/500 [==============================] - 0s 932us/step - loss: 5309917560832.0000 - mae: 1995782.6250\n",
            "Epoch 77/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 5247722323968.0000 - mae: 1984061.7500\n",
            "Epoch 78/200\n",
            "500/500 [==============================] - 0s 926us/step - loss: 5185840087040.0000 - mae: 1972343.6250\n",
            "Epoch 79/200\n",
            "500/500 [==============================] - 0s 903us/step - loss: 5124371513344.0000 - mae: 1960607.8750\n",
            "Epoch 80/200\n",
            "500/500 [==============================] - 0s 922us/step - loss: 5063284621312.0000 - mae: 1948898.7500\n",
            "Epoch 81/200\n",
            "500/500 [==============================] - 0s 932us/step - loss: 5002522263552.0000 - mae: 1937173.0000\n",
            "Epoch 82/200\n",
            "500/500 [==============================] - 0s 983us/step - loss: 4942204502016.0000 - mae: 1925450.5000\n",
            "Epoch 83/200\n",
            "500/500 [==============================] - 0s 934us/step - loss: 4882254790656.0000 - mae: 1913736.1250\n",
            "Epoch 84/200\n",
            "500/500 [==============================] - 0s 878us/step - loss: 4822674178048.0000 - mae: 1902028.6250\n",
            "Epoch 85/200\n",
            "500/500 [==============================] - 0s 941us/step - loss: 4763472625664.0000 - mae: 1890313.7500\n",
            "Epoch 86/200\n",
            "500/500 [==============================] - 0s 995us/step - loss: 4704686309376.0000 - mae: 1878608.0000\n",
            "Epoch 87/200\n",
            "500/500 [==============================] - 0s 972us/step - loss: 4646251790336.0000 - mae: 1866905.3750\n",
            "Epoch 88/200\n",
            "500/500 [==============================] - 0s 939us/step - loss: 4588143902720.0000 - mae: 1855194.3750\n",
            "Epoch 89/200\n",
            "500/500 [==============================] - 0s 965us/step - loss: 4530427133952.0000 - mae: 1843494.0000\n",
            "Epoch 90/200\n",
            "500/500 [==============================] - 0s 930us/step - loss: 4473093619712.0000 - mae: 1831791.5000\n",
            "Epoch 91/200\n",
            "500/500 [==============================] - 0s 935us/step - loss: 4416098271232.0000 - mae: 1820084.0000\n",
            "Epoch 92/200\n",
            "500/500 [==============================] - 0s 900us/step - loss: 4359485390848.0000 - mae: 1808385.6250\n",
            "Epoch 93/200\n",
            "500/500 [==============================] - 0s 999us/step - loss: 4303176335360.0000 - mae: 1796686.6250\n",
            "Epoch 94/200\n",
            "500/500 [==============================] - 0s 900us/step - loss: 4247292477440.0000 - mae: 1784960.2500\n",
            "Epoch 95/200\n",
            "500/500 [==============================] - 0s 973us/step - loss: 4191836438528.0000 - mae: 1773269.7500\n",
            "Epoch 96/200\n",
            "500/500 [==============================] - 0s 935us/step - loss: 4136733507584.0000 - mae: 1761593.8750\n",
            "Epoch 97/200\n",
            "500/500 [==============================] - 0s 954us/step - loss: 4081937022976.0000 - mae: 1749888.3750\n",
            "Epoch 98/200\n",
            "500/500 [==============================] - 0s 950us/step - loss: 4027551580160.0000 - mae: 1738185.5000\n",
            "Epoch 99/200\n",
            "500/500 [==============================] - 0s 893us/step - loss: 3973532090368.0000 - mae: 1726496.8750\n",
            "Epoch 100/200\n",
            "500/500 [==============================] - 0s 943us/step - loss: 3919877242880.0000 - mae: 1714801.8750\n",
            "Epoch 101/200\n",
            "500/500 [==============================] - 0s 893us/step - loss: 3866587299840.0000 - mae: 1703114.7500\n",
            "Epoch 102/200\n",
            "500/500 [==============================] - 0s 933us/step - loss: 3813704728576.0000 - mae: 1691409.8750\n",
            "Epoch 103/200\n",
            "500/500 [==============================] - 0s 913us/step - loss: 3761225072640.0000 - mae: 1679722.8750\n",
            "Epoch 104/200\n",
            "500/500 [==============================] - 0s 960us/step - loss: 3709037969408.0000 - mae: 1668066.7500\n",
            "Epoch 105/200\n",
            "500/500 [==============================] - 0s 916us/step - loss: 3657239887872.0000 - mae: 1656352.3750\n",
            "Epoch 106/200\n",
            "500/500 [==============================] - 0s 926us/step - loss: 3605831090176.0000 - mae: 1644683.2500\n",
            "Epoch 107/200\n",
            "500/500 [==============================] - 0s 986us/step - loss: 3554736340992.0000 - mae: 1632989.2500\n",
            "Epoch 108/200\n",
            "500/500 [==============================] - 0s 934us/step - loss: 3504083304448.0000 - mae: 1621296.3750\n",
            "Epoch 109/200\n",
            "500/500 [==============================] - 0s 904us/step - loss: 3453775511552.0000 - mae: 1609631.1250\n",
            "Epoch 110/200\n",
            "500/500 [==============================] - 0s 898us/step - loss: 3403833409536.0000 - mae: 1597941.0000\n",
            "Epoch 111/200\n",
            "500/500 [==============================] - 0s 978us/step - loss: 3354316767232.0000 - mae: 1586270.5000\n",
            "Epoch 112/200\n",
            "500/500 [==============================] - 0s 933us/step - loss: 3305064366080.0000 - mae: 1574611.0000\n",
            "Epoch 113/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 3256174772224.0000 - mae: 1562911.6250\n",
            "Epoch 114/200\n",
            "500/500 [==============================] - 0s 940us/step - loss: 3207710900224.0000 - mae: 1551236.8750\n",
            "Epoch 115/200\n",
            "500/500 [==============================] - 0s 946us/step - loss: 3159555833856.0000 - mae: 1539568.0000\n",
            "Epoch 116/200\n",
            "500/500 [==============================] - 0s 954us/step - loss: 3111745224704.0000 - mae: 1527869.2500\n",
            "Epoch 117/200\n",
            "500/500 [==============================] - 0s 961us/step - loss: 3064365056000.0000 - mae: 1516187.3750\n",
            "Epoch 118/200\n",
            "500/500 [==============================] - 0s 974us/step - loss: 3017366306816.0000 - mae: 1504525.1250\n",
            "Epoch 119/200\n",
            "500/500 [==============================] - 0s 947us/step - loss: 2970781745152.0000 - mae: 1492842.2500\n",
            "Epoch 120/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 2924527747072.0000 - mae: 1481199.6250\n",
            "Epoch 121/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 2878607720448.0000 - mae: 1469502.8750\n",
            "Epoch 122/200\n",
            "500/500 [==============================] - 0s 986us/step - loss: 2833065705472.0000 - mae: 1457858.8750\n",
            "Epoch 123/200\n",
            "500/500 [==============================] - 0s 982us/step - loss: 2787837476864.0000 - mae: 1446171.1250\n",
            "Epoch 124/200\n",
            "500/500 [==============================] - 0s 964us/step - loss: 2743007182848.0000 - mae: 1434505.7500\n",
            "Epoch 125/200\n",
            "500/500 [==============================] - 0s 966us/step - loss: 2698537336832.0000 - mae: 1422835.8750\n",
            "Epoch 126/200\n",
            "500/500 [==============================] - 0s 953us/step - loss: 2654502125568.0000 - mae: 1411156.8750\n",
            "Epoch 127/200\n",
            "500/500 [==============================] - 0s 946us/step - loss: 2610859606016.0000 - mae: 1399505.0000\n",
            "Epoch 128/200\n",
            "500/500 [==============================] - 0s 951us/step - loss: 2567545290752.0000 - mae: 1387871.2500\n",
            "Epoch 129/200\n",
            "500/500 [==============================] - 0s 936us/step - loss: 2524609511424.0000 - mae: 1376207.8750\n",
            "Epoch 130/200\n",
            "500/500 [==============================] - 0s 906us/step - loss: 2482057248768.0000 - mae: 1364567.0000\n",
            "Epoch 131/200\n",
            "500/500 [==============================] - 0s 940us/step - loss: 2439853899776.0000 - mae: 1352911.7500\n",
            "Epoch 132/200\n",
            "500/500 [==============================] - 0s 955us/step - loss: 2398035378176.0000 - mae: 1341259.2500\n",
            "Epoch 133/200\n",
            "500/500 [==============================] - 0s 934us/step - loss: 2356592771072.0000 - mae: 1329621.3750\n",
            "Epoch 134/200\n",
            "500/500 [==============================] - 0s 991us/step - loss: 2315504582656.0000 - mae: 1317979.3750\n",
            "Epoch 135/200\n",
            "500/500 [==============================] - 0s 909us/step - loss: 2274723627008.0000 - mae: 1306346.0000\n",
            "Epoch 136/200\n",
            "500/500 [==============================] - 0s 926us/step - loss: 2234301546496.0000 - mae: 1294674.5000\n",
            "Epoch 137/200\n",
            "500/500 [==============================] - 0s 929us/step - loss: 2194236506112.0000 - mae: 1283042.5000\n",
            "Epoch 138/200\n",
            "500/500 [==============================] - 0s 941us/step - loss: 2154597580800.0000 - mae: 1271371.3750\n",
            "Epoch 139/200\n",
            "500/500 [==============================] - 0s 915us/step - loss: 2115353968640.0000 - mae: 1259741.1250\n",
            "Epoch 140/200\n",
            "500/500 [==============================] - 0s 932us/step - loss: 2076431876096.0000 - mae: 1248121.2500\n",
            "Epoch 141/200\n",
            "500/500 [==============================] - 0s 926us/step - loss: 2037863677952.0000 - mae: 1236469.3750\n",
            "Epoch 142/200\n",
            "500/500 [==============================] - 0s 943us/step - loss: 1999726313472.0000 - mae: 1224815.7500\n",
            "Epoch 143/200\n",
            "500/500 [==============================] - 0s 942us/step - loss: 1961946906624.0000 - mae: 1213224.3750\n",
            "Epoch 144/200\n",
            "500/500 [==============================] - 0s 926us/step - loss: 1924504748032.0000 - mae: 1201598.8750\n",
            "Epoch 145/200\n",
            "500/500 [==============================] - 0s 928us/step - loss: 1887450562560.0000 - mae: 1189956.1250\n",
            "Epoch 146/200\n",
            "500/500 [==============================] - 0s 934us/step - loss: 1850767310848.0000 - mae: 1178346.0000\n",
            "Epoch 147/200\n",
            "500/500 [==============================] - 0s 965us/step - loss: 1814442541056.0000 - mae: 1166735.1250\n",
            "Epoch 148/200\n",
            "500/500 [==============================] - 0s 961us/step - loss: 1778519375872.0000 - mae: 1155114.6250\n",
            "Epoch 149/200\n",
            "500/500 [==============================] - 0s 981us/step - loss: 1742921400320.0000 - mae: 1143522.2500\n",
            "Epoch 150/200\n",
            "500/500 [==============================] - 0s 973us/step - loss: 1707693178880.0000 - mae: 1131884.0000\n",
            "Epoch 151/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 1672814657536.0000 - mae: 1120291.2500\n",
            "Epoch 152/200\n",
            "500/500 [==============================] - 0s 944us/step - loss: 1638306938880.0000 - mae: 1108666.8750\n",
            "Epoch 153/200\n",
            "500/500 [==============================] - 0s 965us/step - loss: 1604190601216.0000 - mae: 1097056.8750\n",
            "Epoch 154/200\n",
            "500/500 [==============================] - 0s 945us/step - loss: 1570457911296.0000 - mae: 1085461.1250\n",
            "Epoch 155/200\n",
            "500/500 [==============================] - 0s 985us/step - loss: 1537065615360.0000 - mae: 1073862.0000\n",
            "Epoch 156/200\n",
            "500/500 [==============================] - 0s 904us/step - loss: 1504026034176.0000 - mae: 1062259.6250\n",
            "Epoch 157/200\n",
            "500/500 [==============================] - 0s 934us/step - loss: 1471351881728.0000 - mae: 1050658.2500\n",
            "Epoch 158/200\n",
            "500/500 [==============================] - 0s 921us/step - loss: 1439028740096.0000 - mae: 1039066.5000\n",
            "Epoch 159/200\n",
            "500/500 [==============================] - 0s 917us/step - loss: 1407065784320.0000 - mae: 1027469.2500\n",
            "Epoch 160/200\n",
            "500/500 [==============================] - 0s 954us/step - loss: 1375482675200.0000 - mae: 1015881.1875\n",
            "Epoch 161/200\n",
            "500/500 [==============================] - 0s 935us/step - loss: 1344285966336.0000 - mae: 1004275.2500\n",
            "Epoch 162/200\n",
            "500/500 [==============================] - 0s 933us/step - loss: 1313444200448.0000 - mae: 992705.0625\n",
            "Epoch 163/200\n",
            "500/500 [==============================] - 0s 910us/step - loss: 1282963800064.0000 - mae: 981114.1250\n",
            "Epoch 164/200\n",
            "500/500 [==============================] - 0s 946us/step - loss: 1252915544064.0000 - mae: 969545.2500\n",
            "Epoch 165/200\n",
            "500/500 [==============================] - 0s 921us/step - loss: 1223236648960.0000 - mae: 957990.2500\n",
            "Epoch 166/200\n",
            "500/500 [==============================] - 0s 900us/step - loss: 1193871409152.0000 - mae: 946453.8750\n",
            "Epoch 167/200\n",
            "500/500 [==============================] - 0s 927us/step - loss: 1164869107712.0000 - mae: 934862.8125\n",
            "Epoch 168/200\n",
            "500/500 [==============================] - 0s 942us/step - loss: 1136239443968.0000 - mae: 923318.1875\n",
            "Epoch 169/200\n",
            "500/500 [==============================] - 0s 962us/step - loss: 1107965771776.0000 - mae: 911757.1250\n",
            "Epoch 170/200\n",
            "500/500 [==============================] - 0s 977us/step - loss: 1080049205248.0000 - mae: 900207.2500\n",
            "Epoch 171/200\n",
            "500/500 [==============================] - 0s 929us/step - loss: 1052503375872.0000 - mae: 888653.5625\n",
            "Epoch 172/200\n",
            "500/500 [==============================] - 0s 939us/step - loss: 1025312948224.0000 - mae: 877112.1250\n",
            "Epoch 173/200\n",
            "500/500 [==============================] - 0s 935us/step - loss: 998479364096.0000 - mae: 865552.0625\n",
            "Epoch 174/200\n",
            "500/500 [==============================] - 0s 933us/step - loss: 972038471680.0000 - mae: 854015.0000\n",
            "Epoch 175/200\n",
            "500/500 [==============================] - 0s 937us/step - loss: 945996890112.0000 - mae: 842467.6250\n",
            "Epoch 176/200\n",
            "500/500 [==============================] - 0s 943us/step - loss: 920288886784.0000 - mae: 830976.4375\n",
            "Epoch 177/200\n",
            "500/500 [==============================] - 0s 984us/step - loss: 894923702272.0000 - mae: 819433.5000\n",
            "Epoch 178/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 869913591808.0000 - mae: 807927.4375\n",
            "Epoch 179/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 845270876160.0000 - mae: 796380.1250\n",
            "Epoch 180/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 821005713408.0000 - mae: 784881.0625\n",
            "Epoch 181/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 797104340992.0000 - mae: 773366.0000\n",
            "Epoch 182/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 773559091200.0000 - mae: 761875.0000\n",
            "Epoch 183/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 750372061184.0000 - mae: 750358.5000\n",
            "Epoch 184/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 727555178496.0000 - mae: 738868.8750\n",
            "Epoch 185/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 705100972032.0000 - mae: 727383.1250\n",
            "Epoch 186/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 682980409344.0000 - mae: 715893.3125\n",
            "Epoch 187/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 661237923840.0000 - mae: 704407.0000\n",
            "Epoch 188/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 639871680512.0000 - mae: 692914.9375\n",
            "Epoch 189/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 618879844352.0000 - mae: 681471.1250\n",
            "Epoch 190/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 598229516288.0000 - mae: 670014.2500\n",
            "Epoch 191/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 577942323200.0000 - mae: 658561.7500\n",
            "Epoch 192/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 558012628992.0000 - mae: 647109.0000\n",
            "Epoch 193/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 538444136448.0000 - mae: 635650.2500\n",
            "Epoch 194/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 519254802432.0000 - mae: 624221.2500\n",
            "Epoch 195/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 500408713216.0000 - mae: 612807.8125\n",
            "Epoch 196/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 481931427840.0000 - mae: 601380.6250\n",
            "Epoch 197/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 463823011840.0000 - mae: 589976.2500\n",
            "Epoch 198/200\n",
            "500/500 [==============================] - 0s 975us/step - loss: 446050926592.0000 - mae: 578586.1250\n",
            "Epoch 199/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 428653084672.0000 - mae: 567176.6250\n",
            "Epoch 200/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 411630600192.0000 - mae: 555813.1875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MfRAKJk7fR2",
        "colab_type": "text"
      },
      "source": [
        "Scaling it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRCAd0F27hhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_scalar = x[index]/np.max(x)\n",
        "y_scalar = y[index]/np.max(y)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8C-js5b1WX_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9cafbc4-85be-4bde-dc19-9c1c8bc2c800"
      },
      "source": [
        "model1 = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='linear')\n",
        "])\n",
        "model1.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "history = model1.fit(x_scalar, y_scalar, epochs=200, verbose=1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2609 - mae: 0.4096\n",
            "Epoch 2/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.0631 - mae: 0.2152\n",
            "Epoch 3/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.0406 - mae: 0.1738\n",
            "Epoch 4/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.0257 - mae: 0.1378\n",
            "Epoch 5/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.0140 - mae: 0.1016\n",
            "Epoch 6/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.0064 - mae: 0.0684\n",
            "Epoch 7/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.0023 - mae: 0.0412\n",
            "Epoch 8/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 6.4575e-04 - mae: 0.0214\n",
            "Epoch 9/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 1.2419e-04 - mae: 0.0093\n",
            "Epoch 10/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 1.5009e-05 - mae: 0.0032\n",
            "Epoch 11/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 1.0018e-06 - mae: 7.8648e-04\n",
            "Epoch 12/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 3.1223e-08 - mae: 1.3252e-04\n",
            "Epoch 13/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 3.6187e-10 - mae: 1.3354e-05\n",
            "Epoch 14/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 1.6239e-12 - mae: 9.9463e-07\n",
            "Epoch 15/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 4.4513e-13 - mae: 5.7195e-07\n",
            "Epoch 16/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 2.8880e-13 - mae: 4.5850e-07\n",
            "Epoch 17/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 1.6318e-13 - mae: 3.4199e-07\n",
            "Epoch 18/200\n",
            "500/500 [==============================] - 0s 972us/step - loss: 1.0173e-13 - mae: 2.6552e-07\n",
            "Epoch 19/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 7.0618e-14 - mae: 2.1964e-07\n",
            "Epoch 20/200\n",
            "500/500 [==============================] - 0s 951us/step - loss: 4.7871e-14 - mae: 1.7843e-07\n",
            "Epoch 21/200\n",
            "500/500 [==============================] - 0s 976us/step - loss: 3.5640e-14 - mae: 1.5150e-07\n",
            "Epoch 22/200\n",
            "500/500 [==============================] - 0s 986us/step - loss: 2.7262e-14 - mae: 1.2994e-07\n",
            "Epoch 23/200\n",
            "500/500 [==============================] - 0s 958us/step - loss: 2.4588e-14 - mae: 1.2238e-07\n",
            "Epoch 24/200\n",
            "500/500 [==============================] - 0s 954us/step - loss: 2.2267e-14 - mae: 1.1494e-07\n",
            "Epoch 25/200\n",
            "500/500 [==============================] - 0s 925us/step - loss: 2.1646e-14 - mae: 1.1215e-07\n",
            "Epoch 26/200\n",
            "500/500 [==============================] - 0s 929us/step - loss: 2.1308e-14 - mae: 1.1056e-07\n",
            "Epoch 27/200\n",
            "500/500 [==============================] - 0s 921us/step - loss: 2.1767e-14 - mae: 1.1261e-07\n",
            "Epoch 28/200\n",
            "500/500 [==============================] - 0s 960us/step - loss: 2.1471e-14 - mae: 1.1128e-07\n",
            "Epoch 29/200\n",
            "500/500 [==============================] - 0s 925us/step - loss: 2.2738e-14 - mae: 1.1593e-07\n",
            "Epoch 30/200\n",
            "500/500 [==============================] - 0s 925us/step - loss: 2.3780e-14 - mae: 1.1923e-07\n",
            "Epoch 31/200\n",
            "500/500 [==============================] - 0s 977us/step - loss: 2.4836e-14 - mae: 1.2191e-07\n",
            "Epoch 32/200\n",
            "500/500 [==============================] - 0s 930us/step - loss: 2.5844e-14 - mae: 1.2583e-07\n",
            "Epoch 33/200\n",
            "500/500 [==============================] - 0s 956us/step - loss: 2.8402e-14 - mae: 1.3183e-07\n",
            "Epoch 34/200\n",
            "500/500 [==============================] - 0s 953us/step - loss: 3.1919e-14 - mae: 1.4117e-07\n",
            "Epoch 35/200\n",
            "500/500 [==============================] - 0s 927us/step - loss: 5.2521e-14 - mae: 1.7720e-07\n",
            "Epoch 36/200\n",
            "500/500 [==============================] - 0s 905us/step - loss: 1.2163e-13 - mae: 2.5873e-07\n",
            "Epoch 37/200\n",
            "500/500 [==============================] - 0s 939us/step - loss: 1.0867e-09 - mae: 1.4499e-05\n",
            "Epoch 38/200\n",
            "500/500 [==============================] - 0s 915us/step - loss: 2.5319e-09 - mae: 2.3248e-05\n",
            "Epoch 39/200\n",
            "500/500 [==============================] - 0s 985us/step - loss: 8.0789e-09 - mae: 2.8088e-05\n",
            "Epoch 40/200\n",
            "500/500 [==============================] - 0s 936us/step - loss: 6.6698e-14 - mae: 2.0520e-07\n",
            "Epoch 41/200\n",
            "500/500 [==============================] - 0s 904us/step - loss: 3.0562e-13 - mae: 3.7260e-07\n",
            "Epoch 42/200\n",
            "500/500 [==============================] - 0s 967us/step - loss: 1.9333e-09 - mae: 1.8911e-05\n",
            "Epoch 43/200\n",
            "500/500 [==============================] - 0s 943us/step - loss: 2.3454e-09 - mae: 1.7077e-05\n",
            "Epoch 44/200\n",
            "500/500 [==============================] - 0s 940us/step - loss: 2.3926e-09 - mae: 1.5020e-05\n",
            "Epoch 45/200\n",
            "500/500 [==============================] - 0s 915us/step - loss: 1.9523e-09 - mae: 1.6518e-05\n",
            "Epoch 46/200\n",
            "500/500 [==============================] - 0s 966us/step - loss: 1.6803e-09 - mae: 1.4474e-05\n",
            "Epoch 47/200\n",
            "500/500 [==============================] - 0s 939us/step - loss: 1.7401e-09 - mae: 2.0822e-05\n",
            "Epoch 48/200\n",
            "500/500 [==============================] - 0s 983us/step - loss: 3.2710e-09 - mae: 1.6776e-05\n",
            "Epoch 49/200\n",
            "500/500 [==============================] - 0s 925us/step - loss: 3.7418e-09 - mae: 2.4509e-05\n",
            "Epoch 50/200\n",
            "500/500 [==============================] - 0s 938us/step - loss: 2.6872e-11 - mae: 1.8141e-06\n",
            "Epoch 51/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 5.3481e-09 - mae: 3.3480e-05\n",
            "Epoch 52/200\n",
            "500/500 [==============================] - 0s 966us/step - loss: 6.9240e-12 - mae: 9.5991e-07\n",
            "Epoch 53/200\n",
            "500/500 [==============================] - 0s 908us/step - loss: 2.1547e-09 - mae: 1.5655e-05\n",
            "Epoch 54/200\n",
            "500/500 [==============================] - 0s 956us/step - loss: 3.0738e-09 - mae: 2.3706e-05\n",
            "Epoch 55/200\n",
            "500/500 [==============================] - 0s 906us/step - loss: 2.7610e-09 - mae: 1.9850e-05\n",
            "Epoch 56/200\n",
            "500/500 [==============================] - 0s 932us/step - loss: 4.1052e-12 - mae: 9.5403e-07\n",
            "Epoch 57/200\n",
            "500/500 [==============================] - 0s 962us/step - loss: 2.4402e-09 - mae: 1.3778e-05\n",
            "Epoch 58/200\n",
            "500/500 [==============================] - 0s 970us/step - loss: 4.5962e-09 - mae: 3.1500e-05\n",
            "Epoch 59/200\n",
            "500/500 [==============================] - 0s 924us/step - loss: 8.2414e-12 - mae: 1.0832e-06\n",
            "Epoch 60/200\n",
            "500/500 [==============================] - 0s 947us/step - loss: 2.0042e-09 - mae: 3.0087e-05\n",
            "Epoch 61/200\n",
            "500/500 [==============================] - 0s 929us/step - loss: 2.5164e-09 - mae: 2.0894e-05\n",
            "Epoch 62/200\n",
            "500/500 [==============================] - 0s 932us/step - loss: 1.0861e-09 - mae: 1.4447e-05\n",
            "Epoch 63/200\n",
            "500/500 [==============================] - 0s 967us/step - loss: 3.7203e-09 - mae: 2.0513e-05\n",
            "Epoch 64/200\n",
            "500/500 [==============================] - 0s 939us/step - loss: 1.2763e-09 - mae: 1.8237e-05\n",
            "Epoch 65/200\n",
            "500/500 [==============================] - 0s 947us/step - loss: 1.7567e-09 - mae: 2.2303e-05\n",
            "Epoch 66/200\n",
            "500/500 [==============================] - 0s 938us/step - loss: 2.7119e-09 - mae: 1.9551e-05\n",
            "Epoch 67/200\n",
            "500/500 [==============================] - 0s 949us/step - loss: 1.6519e-09 - mae: 1.1629e-05\n",
            "Epoch 68/200\n",
            "500/500 [==============================] - 0s 936us/step - loss: 2.1951e-09 - mae: 2.0000e-05\n",
            "Epoch 69/200\n",
            "500/500 [==============================] - 0s 945us/step - loss: 4.5736e-09 - mae: 2.3179e-05\n",
            "Epoch 70/200\n",
            "500/500 [==============================] - 0s 930us/step - loss: 5.8879e-13 - mae: 5.1007e-07\n",
            "Epoch 71/200\n",
            "500/500 [==============================] - 0s 970us/step - loss: 2.6688e-09 - mae: 1.5326e-05\n",
            "Epoch 72/200\n",
            "500/500 [==============================] - 0s 946us/step - loss: 1.9670e-09 - mae: 1.3123e-05\n",
            "Epoch 73/200\n",
            "500/500 [==============================] - 0s 947us/step - loss: 2.9396e-09 - mae: 2.4205e-05\n",
            "Epoch 74/200\n",
            "500/500 [==============================] - 0s 950us/step - loss: 3.3850e-09 - mae: 1.9157e-05\n",
            "Epoch 75/200\n",
            "500/500 [==============================] - 0s 928us/step - loss: 9.5489e-10 - mae: 1.1647e-05\n",
            "Epoch 76/200\n",
            "500/500 [==============================] - 0s 925us/step - loss: 3.0208e-09 - mae: 2.2727e-05\n",
            "Epoch 77/200\n",
            "500/500 [==============================] - 0s 949us/step - loss: 2.3914e-09 - mae: 1.6291e-05\n",
            "Epoch 78/200\n",
            "500/500 [==============================] - 0s 947us/step - loss: 5.4345e-09 - mae: 2.0991e-05\n",
            "Epoch 79/200\n",
            "500/500 [==============================] - 0s 904us/step - loss: 9.9248e-13 - mae: 3.7881e-07\n",
            "Epoch 80/200\n",
            "500/500 [==============================] - 0s 949us/step - loss: 6.5635e-10 - mae: 9.7735e-06\n",
            "Epoch 81/200\n",
            "500/500 [==============================] - 0s 964us/step - loss: 2.2701e-09 - mae: 2.7728e-05\n",
            "Epoch 82/200\n",
            "500/500 [==============================] - 0s 954us/step - loss: 1.7954e-09 - mae: 1.5743e-05\n",
            "Epoch 83/200\n",
            "500/500 [==============================] - 0s 940us/step - loss: 2.7936e-09 - mae: 3.4717e-05\n",
            "Epoch 84/200\n",
            "500/500 [==============================] - 0s 945us/step - loss: 1.9148e-09 - mae: 2.2181e-05\n",
            "Epoch 85/200\n",
            "500/500 [==============================] - 0s 947us/step - loss: 8.7987e-10 - mae: 9.6712e-06\n",
            "Epoch 86/200\n",
            "500/500 [==============================] - 0s 896us/step - loss: 2.6788e-09 - mae: 1.9093e-05\n",
            "Epoch 87/200\n",
            "500/500 [==============================] - 0s 932us/step - loss: 3.8286e-09 - mae: 2.1753e-05\n",
            "Epoch 88/200\n",
            "500/500 [==============================] - 0s 927us/step - loss: 9.9848e-10 - mae: 1.1568e-05\n",
            "Epoch 89/200\n",
            "500/500 [==============================] - 0s 902us/step - loss: 1.2347e-09 - mae: 2.1248e-05\n",
            "Epoch 90/200\n",
            "500/500 [==============================] - 0s 897us/step - loss: 1.6388e-09 - mae: 1.7479e-05\n",
            "Epoch 91/200\n",
            "500/500 [==============================] - 0s 947us/step - loss: 2.3982e-09 - mae: 2.4727e-05\n",
            "Epoch 92/200\n",
            "500/500 [==============================] - 0s 963us/step - loss: 2.0016e-09 - mae: 1.4691e-05\n",
            "Epoch 93/200\n",
            "500/500 [==============================] - 0s 953us/step - loss: 1.7672e-09 - mae: 2.3771e-05\n",
            "Epoch 94/200\n",
            "500/500 [==============================] - 0s 966us/step - loss: 1.8156e-09 - mae: 1.8049e-05\n",
            "Epoch 95/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 2.9675e-09 - mae: 1.9740e-05\n",
            "Epoch 96/200\n",
            "500/500 [==============================] - 0s 956us/step - loss: 3.9927e-09 - mae: 1.8493e-05\n",
            "Epoch 97/200\n",
            "500/500 [==============================] - 0s 921us/step - loss: 2.2482e-10 - mae: 2.7915e-06\n",
            "Epoch 98/200\n",
            "500/500 [==============================] - 0s 934us/step - loss: 3.5063e-09 - mae: 2.5407e-05\n",
            "Epoch 99/200\n",
            "500/500 [==============================] - 0s 988us/step - loss: 2.3730e-09 - mae: 2.2129e-05\n",
            "Epoch 100/200\n",
            "500/500 [==============================] - 0s 950us/step - loss: 1.5336e-09 - mae: 1.2744e-05\n",
            "Epoch 101/200\n",
            "500/500 [==============================] - 0s 960us/step - loss: 2.4416e-09 - mae: 2.0913e-05\n",
            "Epoch 102/200\n",
            "500/500 [==============================] - 0s 987us/step - loss: 8.6956e-10 - mae: 9.6976e-06\n",
            "Epoch 103/200\n",
            "500/500 [==============================] - 0s 936us/step - loss: 1.6260e-09 - mae: 1.7807e-05\n",
            "Epoch 104/200\n",
            "500/500 [==============================] - 0s 950us/step - loss: 2.7679e-09 - mae: 2.5861e-05\n",
            "Epoch 105/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 1.2097e-09 - mae: 1.6520e-05\n",
            "Epoch 106/200\n",
            "500/500 [==============================] - 0s 982us/step - loss: 2.4559e-09 - mae: 3.0392e-05\n",
            "Epoch 107/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 1.3152e-09 - mae: 1.8991e-05\n",
            "Epoch 108/200\n",
            "500/500 [==============================] - 0s 965us/step - loss: 2.3303e-09 - mae: 1.6218e-05\n",
            "Epoch 109/200\n",
            "500/500 [==============================] - 0s 928us/step - loss: 1.4698e-09 - mae: 2.2627e-05\n",
            "Epoch 110/200\n",
            "500/500 [==============================] - 0s 941us/step - loss: 3.3019e-09 - mae: 2.9373e-05\n",
            "Epoch 111/200\n",
            "500/500 [==============================] - 0s 926us/step - loss: 2.9300e-09 - mae: 1.4879e-05\n",
            "Epoch 112/200\n",
            "500/500 [==============================] - 0s 950us/step - loss: 3.0775e-12 - mae: 8.3825e-07\n",
            "Epoch 113/200\n",
            "500/500 [==============================] - 0s 968us/step - loss: 2.4377e-09 - mae: 2.6857e-05\n",
            "Epoch 114/200\n",
            "500/500 [==============================] - 0s 903us/step - loss: 1.7770e-09 - mae: 2.9683e-05\n",
            "Epoch 115/200\n",
            "500/500 [==============================] - 0s 961us/step - loss: 1.8095e-09 - mae: 1.5627e-05\n",
            "Epoch 116/200\n",
            "500/500 [==============================] - 0s 968us/step - loss: 2.8413e-09 - mae: 1.9549e-05\n",
            "Epoch 117/200\n",
            "500/500 [==============================] - 0s 897us/step - loss: 1.5354e-09 - mae: 1.6825e-05\n",
            "Epoch 118/200\n",
            "500/500 [==============================] - 0s 990us/step - loss: 2.9789e-09 - mae: 1.5435e-05\n",
            "Epoch 119/200\n",
            "500/500 [==============================] - 0s 935us/step - loss: 1.2402e-09 - mae: 1.3660e-05\n",
            "Epoch 120/200\n",
            "500/500 [==============================] - 0s 941us/step - loss: 2.5285e-09 - mae: 1.9729e-05\n",
            "Epoch 121/200\n",
            "500/500 [==============================] - 0s 924us/step - loss: 1.7206e-09 - mae: 1.7477e-05\n",
            "Epoch 122/200\n",
            "500/500 [==============================] - 0s 933us/step - loss: 5.0896e-09 - mae: 2.5957e-05\n",
            "Epoch 123/200\n",
            "500/500 [==============================] - 0s 941us/step - loss: 1.8264e-13 - mae: 3.2376e-07\n",
            "Epoch 124/200\n",
            "500/500 [==============================] - 0s 932us/step - loss: 2.0634e-09 - mae: 2.5112e-05\n",
            "Epoch 125/200\n",
            "500/500 [==============================] - 0s 922us/step - loss: 1.5913e-09 - mae: 1.8019e-05\n",
            "Epoch 126/200\n",
            "500/500 [==============================] - 0s 919us/step - loss: 3.4314e-09 - mae: 1.8137e-05\n",
            "Epoch 127/200\n",
            "500/500 [==============================] - 0s 940us/step - loss: 1.5520e-09 - mae: 1.3307e-05\n",
            "Epoch 128/200\n",
            "500/500 [==============================] - 0s 937us/step - loss: 1.0038e-08 - mae: 2.7989e-05\n",
            "Epoch 129/200\n",
            "500/500 [==============================] - 0s 905us/step - loss: 5.1942e-14 - mae: 1.8191e-07\n",
            "Epoch 130/200\n",
            "500/500 [==============================] - 0s 922us/step - loss: 3.6377e-12 - mae: 8.3777e-07\n",
            "Epoch 131/200\n",
            "500/500 [==============================] - 0s 942us/step - loss: 1.5529e-09 - mae: 2.0483e-05\n",
            "Epoch 132/200\n",
            "500/500 [==============================] - 0s 929us/step - loss: 3.5581e-09 - mae: 3.5261e-05\n",
            "Epoch 133/200\n",
            "500/500 [==============================] - 0s 949us/step - loss: 8.1179e-10 - mae: 1.1804e-05\n",
            "Epoch 134/200\n",
            "500/500 [==============================] - 0s 953us/step - loss: 3.4212e-09 - mae: 2.6290e-05\n",
            "Epoch 135/200\n",
            "500/500 [==============================] - 0s 896us/step - loss: 7.8951e-10 - mae: 9.8315e-06\n",
            "Epoch 136/200\n",
            "500/500 [==============================] - 0s 932us/step - loss: 3.2283e-09 - mae: 2.4161e-05\n",
            "Epoch 137/200\n",
            "500/500 [==============================] - 0s 943us/step - loss: 1.7748e-09 - mae: 1.4262e-05\n",
            "Epoch 138/200\n",
            "500/500 [==============================] - 0s 914us/step - loss: 5.4498e-09 - mae: 2.4849e-05\n",
            "Epoch 139/200\n",
            "500/500 [==============================] - 0s 957us/step - loss: 5.6262e-11 - mae: 1.8854e-06\n",
            "Epoch 140/200\n",
            "500/500 [==============================] - 0s 913us/step - loss: 1.2808e-09 - mae: 1.1759e-05\n",
            "Epoch 141/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 9.2393e-10 - mae: 1.6406e-05\n",
            "Epoch 142/200\n",
            "500/500 [==============================] - 0s 933us/step - loss: 3.1161e-09 - mae: 2.4620e-05\n",
            "Epoch 143/200\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 1.2822e-09 - mae: 2.1972e-05\n",
            "Epoch 144/200\n",
            "500/500 [==============================] - 0s 911us/step - loss: 1.5442e-09 - mae: 2.2725e-05\n",
            "Epoch 145/200\n",
            "500/500 [==============================] - 0s 952us/step - loss: 3.7316e-09 - mae: 2.4936e-05\n",
            "Epoch 146/200\n",
            "500/500 [==============================] - 0s 954us/step - loss: 9.1258e-10 - mae: 1.2400e-05\n",
            "Epoch 147/200\n",
            "500/500 [==============================] - 0s 894us/step - loss: 4.7827e-09 - mae: 2.9968e-05\n",
            "Epoch 148/200\n",
            "500/500 [==============================] - 0s 922us/step - loss: 7.7714e-14 - mae: 2.1502e-07\n",
            "Epoch 149/200\n",
            "500/500 [==============================] - 0s 937us/step - loss: 1.9638e-09 - mae: 1.3084e-05\n",
            "Epoch 150/200\n",
            "500/500 [==============================] - 0s 968us/step - loss: 2.1145e-09 - mae: 1.5924e-05\n",
            "Epoch 151/200\n",
            "500/500 [==============================] - 0s 945us/step - loss: 2.1049e-09 - mae: 1.3197e-05\n",
            "Epoch 152/200\n",
            "500/500 [==============================] - 0s 984us/step - loss: 2.8308e-09 - mae: 2.7561e-05\n",
            "Epoch 153/200\n",
            "500/500 [==============================] - 0s 960us/step - loss: 4.3142e-09 - mae: 2.1145e-05\n",
            "Epoch 154/200\n",
            "500/500 [==============================] - 0s 943us/step - loss: 1.2018e-13 - mae: 2.7116e-07\n",
            "Epoch 155/200\n",
            "500/500 [==============================] - 0s 898us/step - loss: 2.5368e-09 - mae: 1.3626e-05\n",
            "Epoch 156/200\n",
            "500/500 [==============================] - 0s 933us/step - loss: 2.8840e-09 - mae: 1.6371e-05\n",
            "Epoch 157/200\n",
            "500/500 [==============================] - 0s 932us/step - loss: 9.9774e-10 - mae: 1.5201e-05\n",
            "Epoch 158/200\n",
            "500/500 [==============================] - 0s 945us/step - loss: 1.6081e-09 - mae: 2.2229e-05\n",
            "Epoch 159/200\n",
            "500/500 [==============================] - 0s 898us/step - loss: 2.2205e-09 - mae: 1.5553e-05\n",
            "Epoch 160/200\n",
            "500/500 [==============================] - 0s 905us/step - loss: 2.5904e-09 - mae: 2.3012e-05\n",
            "Epoch 161/200\n",
            "500/500 [==============================] - 0s 931us/step - loss: 2.7163e-09 - mae: 2.1969e-05\n",
            "Epoch 162/200\n",
            "500/500 [==============================] - 0s 951us/step - loss: 8.7126e-10 - mae: 1.0308e-05\n",
            "Epoch 163/200\n",
            "500/500 [==============================] - 0s 990us/step - loss: 3.4323e-09 - mae: 2.7469e-05\n",
            "Epoch 164/200\n",
            "500/500 [==============================] - 0s 960us/step - loss: 7.5718e-10 - mae: 7.8480e-06\n",
            "Epoch 165/200\n",
            "500/500 [==============================] - 0s 961us/step - loss: 1.6140e-09 - mae: 1.9676e-05\n",
            "Epoch 166/200\n",
            "500/500 [==============================] - 0s 924us/step - loss: 2.9288e-09 - mae: 3.0174e-05\n",
            "Epoch 167/200\n",
            "500/500 [==============================] - 0s 936us/step - loss: 4.2289e-09 - mae: 2.0941e-05\n",
            "Epoch 168/200\n",
            "500/500 [==============================] - 0s 881us/step - loss: 4.6477e-13 - mae: 4.9596e-07\n",
            "Epoch 169/200\n",
            "500/500 [==============================] - 0s 950us/step - loss: 4.4185e-09 - mae: 1.9788e-05\n",
            "Epoch 170/200\n",
            "500/500 [==============================] - 0s 922us/step - loss: 7.0232e-10 - mae: 6.8525e-06\n",
            "Epoch 171/200\n",
            "500/500 [==============================] - 0s 954us/step - loss: 2.5666e-09 - mae: 2.5335e-05\n",
            "Epoch 172/200\n",
            "500/500 [==============================] - 0s 894us/step - loss: 1.9985e-09 - mae: 1.3008e-05\n",
            "Epoch 173/200\n",
            "500/500 [==============================] - 0s 939us/step - loss: 1.3656e-09 - mae: 1.1468e-05\n",
            "Epoch 174/200\n",
            "500/500 [==============================] - 0s 944us/step - loss: 1.7711e-09 - mae: 1.9300e-05\n",
            "Epoch 175/200\n",
            "500/500 [==============================] - 0s 949us/step - loss: 2.9990e-09 - mae: 2.6760e-05\n",
            "Epoch 176/200\n",
            "500/500 [==============================] - 0s 930us/step - loss: 2.5159e-09 - mae: 1.8035e-05\n",
            "Epoch 177/200\n",
            "500/500 [==============================] - 0s 894us/step - loss: 3.6936e-09 - mae: 1.8737e-05\n",
            "Epoch 178/200\n",
            "500/500 [==============================] - 0s 905us/step - loss: 2.1217e-12 - mae: 7.5555e-07\n",
            "Epoch 179/200\n",
            "500/500 [==============================] - 0s 911us/step - loss: 1.7180e-09 - mae: 1.2325e-05\n",
            "Epoch 180/200\n",
            "500/500 [==============================] - 0s 966us/step - loss: 2.8951e-09 - mae: 2.6859e-05\n",
            "Epoch 181/200\n",
            "500/500 [==============================] - 0s 897us/step - loss: 2.5273e-09 - mae: 2.7732e-05\n",
            "Epoch 182/200\n",
            "500/500 [==============================] - 0s 922us/step - loss: 1.6870e-09 - mae: 1.6830e-05\n",
            "Epoch 183/200\n",
            "500/500 [==============================] - 0s 948us/step - loss: 2.2618e-09 - mae: 1.8439e-05\n",
            "Epoch 184/200\n",
            "500/500 [==============================] - 0s 951us/step - loss: 2.3308e-09 - mae: 1.7942e-05\n",
            "Epoch 185/200\n",
            "500/500 [==============================] - 0s 955us/step - loss: 1.3448e-09 - mae: 1.7986e-05\n",
            "Epoch 186/200\n",
            "500/500 [==============================] - 0s 954us/step - loss: 1.4751e-09 - mae: 2.7957e-05\n",
            "Epoch 187/200\n",
            "500/500 [==============================] - 0s 971us/step - loss: 2.4378e-09 - mae: 3.1451e-05\n",
            "Epoch 188/200\n",
            "500/500 [==============================] - 0s 927us/step - loss: 3.3656e-09 - mae: 2.0457e-05\n",
            "Epoch 189/200\n",
            "500/500 [==============================] - 0s 990us/step - loss: 3.1060e-11 - mae: 1.4138e-06\n",
            "Epoch 190/200\n",
            "500/500 [==============================] - 0s 962us/step - loss: 4.2027e-09 - mae: 2.2638e-05\n",
            "Epoch 191/200\n",
            "500/500 [==============================] - 0s 908us/step - loss: 1.7057e-09 - mae: 1.3927e-05\n",
            "Epoch 192/200\n",
            "500/500 [==============================] - 0s 979us/step - loss: 1.1728e-09 - mae: 1.6152e-05\n",
            "Epoch 193/200\n",
            "500/500 [==============================] - 0s 930us/step - loss: 3.3482e-09 - mae: 2.5948e-05\n",
            "Epoch 194/200\n",
            "500/500 [==============================] - 0s 934us/step - loss: 1.9197e-09 - mae: 1.4078e-05\n",
            "Epoch 195/200\n",
            "500/500 [==============================] - 0s 939us/step - loss: 1.3464e-09 - mae: 1.3677e-05\n",
            "Epoch 196/200\n",
            "500/500 [==============================] - 0s 910us/step - loss: 3.0644e-09 - mae: 2.2626e-05\n",
            "Epoch 197/200\n",
            "500/500 [==============================] - 0s 956us/step - loss: 2.1245e-09 - mae: 1.8517e-05\n",
            "Epoch 198/200\n",
            "500/500 [==============================] - 0s 906us/step - loss: 1.7092e-09 - mae: 1.8416e-05\n",
            "Epoch 199/200\n",
            "500/500 [==============================] - 0s 925us/step - loss: 1.0276e-09 - mae: 1.5163e-05\n",
            "Epoch 200/200\n",
            "500/500 [==============================] - 0s 902us/step - loss: 2.0376e-09 - mae: 2.1668e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG71e9RCDo-T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ed9e77d-cc2e-4c04-b941-c43eb3d68b87"
      },
      "source": [
        "y_pred = model.predict(x_scalar) * np.max(y)\n",
        "mae = np.mean(np.abs(y - y_pred))\n",
        "print(mae)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "857655477.0645261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGqIz2YBfbJm",
        "colab_type": "text"
      },
      "source": [
        "So basically scaling it down is somehow giving poor results so we won't scale anything down"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIAGCw3bJlsM",
        "colab_type": "text"
      },
      "source": [
        "Let us work it out now, Sampling and taking a 2 * root(n) sample use that to initialize the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ejayFIXI37M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = np.array(range(y.shape[0]))\n",
        "np.random.shuffle(index)\n",
        "\n",
        "x = x[index]\n",
        "y = y[index]\n",
        "\n",
        "set_1 = np.array([x[:int(x.shape[0]**0.5)], y[:int(x.shape[0]**0.5)]]).T\n",
        "set_2 = np.array([x[-int(x.shape[0]**0.5):], y[-int(x.shape[0]**0.5):]]).T"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itJgd5lUKjUT",
        "colab_type": "text"
      },
      "source": [
        "Now the equation of line is given by y = mx +c\n",
        "\n",
        "so m = (y1 - y2)/(x1 - x2)\n",
        "\n",
        "   c = y - mx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubRHYJgRJ3Oo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = np.array([(i[1]-j[1])/(i[0]-j[0]) for i,j in zip(set_1, set_2)])\n",
        "c = np.array([set_i[1] - m_i * set_i[0] for set_i, m_i in zip(set_1, m)])\n",
        "\n",
        "mae_anal_set_1 = np.array([sum([abs(set_i[1] - (set_i[0]*m_i + c_i)) for set_i in set_1])/set_1.shape[0] for m_i, c_i in zip(m, c)])\n",
        "\n",
        "sorted_index = np.argsort(mae_anal_set_1)\n",
        "m = m[sorted_index]\n",
        "c = c[sorted_index]\n",
        "mae_anal_set_1 = mae_anal_set_1[sorted_index]\n",
        "\n",
        "final_m, final_c = np.mean(m[:int(m.shape[0]**0.5)]), np.mean(c[:int(m.shape[0]**0.5)])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymkKb07wTeok",
        "colab_type": "text"
      },
      "source": [
        "Now Let us Initialize the weights accoridng to above method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHzo1JR9LA8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2c6c69d-146a-4e77-cf9d-2a69cb04322f"
      },
      "source": [
        "print(model.get_weights())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[98.819244]], dtype=float32), array([97.88447], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5TzvNUqSb5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = [np.array([[final_m]]), np.array([final_c])]\n",
        "model.set_weights(weights)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1GVjeNCT4W0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "f856e20c-5f4b-4143-90f4-7f4b7764f0a2"
      },
      "source": [
        "history = model.fit(x, y, epochs=10, verbose=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "500/500 [==============================] - 0s 972us/step - loss: 63074.4922 - mae: 216.3401\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 0s 945us/step - loss: 64897.6211 - mae: 220.6285\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 0s 909us/step - loss: 64897.6250 - mae: 220.6285\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 0s 927us/step - loss: 64897.6172 - mae: 220.6285\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 0s 935us/step - loss: 64897.6328 - mae: 220.6286\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 0s 941us/step - loss: 64897.6445 - mae: 220.6285\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 0s 957us/step - loss: 64897.6133 - mae: 220.6285\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 0s 906us/step - loss: 64897.6211 - mae: 220.6286\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 0s 954us/step - loss: 64897.6055 - mae: 220.6285\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 0s 990us/step - loss: 55964.0625 - mae: 204.4817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly1do-9ZVTG-",
        "colab_type": "text"
      },
      "source": [
        "Now Let us Create a Workable class for this where input is x and y. Output is weights which can directly be done as model.set_weights(weights)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5XxQQzrVBNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear_weight_initializer:\n",
        "  def __init__(self, x, y, sample_exp=0.5):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.sample_exp = sample_exp\n",
        "    self.set_1 = None\n",
        "    self.set_2 = None\n",
        "    self.final_c = 0\n",
        "    self.final_m = 0\n",
        "  \n",
        "  def setter(self):\n",
        "    index = np.array(range(y.shape[0]))\n",
        "    np.random.shuffle(index)\n",
        "\n",
        "    self.x = self.x[index]\n",
        "    self.y = self.y[index]\n",
        "\n",
        "    self.set_1 = np.array([self.x[:int(self.x.shape[0]**self.sample_exp)], self.y[:int(self.x.shape[0]**self.sample_exp)]]).T\n",
        "    self.set_2 = np.array([self.x[-int(self.x.shape[0]**self.sample_exp):], self.y[-int(self.x.shape[0]**self.sample_exp):]]).T\n",
        "\n",
        "  def m_c_calculator(self):\n",
        "    m = np.array([(i[1]-j[1])/(i[0]-j[0]) for i,j in zip(self.set_1, self.set_2)])\n",
        "    c = np.array([set_i[1] - m_i * set_i[0] for set_i, m_i in zip(self.set_1, m)])\n",
        "\n",
        "    mae_anal_set_1 = np.array([sum([abs(set_i[1] - (set_i[0]*m_i + c_i)) for set_i in self.set_1])/self.set_1.shape[0] for m_i, c_i in zip(m, c)])\n",
        "\n",
        "    sorted_index = np.argsort(mae_anal_set_1)\n",
        "    m = m[sorted_index]\n",
        "    c = c[sorted_index]\n",
        "    mae_anal_set_1 = mae_anal_set_1[sorted_index]\n",
        "\n",
        "    self.final_m, self.final_c = np.mean(m[:int(m.shape[0]**self.sample_exp)]), np.mean(c[:int(m.shape[0]**self.sample_exp)])\n",
        "\n",
        "  def weights_calc(self):\n",
        "    self.setter()\n",
        "    self.m_c_calculator()\n",
        "    weights = [np.array([[self.final_m]]), np.array([self.final_c])]\n",
        "    return weights"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b2y_aHRXz1n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7666096a-79b7-4e50-f749-2e84950e70cc"
      },
      "source": [
        "lwi = Linear_weight_initializer(x, y)\n",
        "weights = lwi.weights_calc()\n",
        "print(weights)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[121.70940038]]), array([891.23464168])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epvTeF5MX_8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "72ef26ac-dac4-4384-edc4-ee1f59a59ea3"
      },
      "source": [
        "model.set_weights(weights)\n",
        "history = model.fit(x, y, epochs=10, verbose=1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.6949 - mae: 0.6155\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.6949 - mae: 0.6155\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.6949 - mae: 0.6155\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.6949 - mae: 0.6155\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.6949 - mae: 0.6155\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.6949 - mae: 0.6155\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.6949 - mae: 0.6155\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 0s 972us/step - loss: 0.6949 - mae: 0.6155\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 0s 952us/step - loss: 0.6949 - mae: 0.6155\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 0s 972us/step - loss: 0.6949 - mae: 0.6155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSPMZFdVYLke",
        "colab_type": "text"
      },
      "source": [
        "Now Let us create a new Dataset with varying degrees of noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCQMZSGmYJdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def noisy_linear_dataset(n):\n",
        "  m = random.random()*1000\n",
        "  c = random.random()*1000\n",
        "\n",
        "  x = np.array([i for i in range(0, 50000, 3)][:16000])\n",
        "  y = np.array([m*i+c+random.randint(-n, n)*random.random() for i in range(0, 50000, 3)][:16000])\n",
        "\n",
        "  return x, y"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY_VkaZOY1ZR",
        "colab_type": "text"
      },
      "source": [
        "We will now run the model for 10 epochs one where tensorflow starts from the beginning and another where it is pre-optimised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEytIzC9mVPs",
        "colab_type": "text"
      },
      "source": [
        "The outputs which contain Function are the outputs from the class above and the ones with Normal are the ones which tensorflow does in the same number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5_f_eFaY0j5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "215b8838-3543-4f2d-a5f8-0927cbc9fd0c"
      },
      "source": [
        "for noise in range(0, 100, 2):\n",
        "  x, y = noisy_linear_dataset(noise)\n",
        "\n",
        "  #Normally\n",
        "\n",
        "  model0 = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='linear', input_shape=(1,))\n",
        "  ])\n",
        "  model0.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "  history = model0.fit(x, y, epochs=10, verbose=0)\n",
        "  mse0, mae0 = model0.evaluate(x, y, verbose=0)\n",
        "\n",
        "  mse0, mae0 = \"{:.4f}\".format(mse0), \"{:.4f}\".format(mae0)\n",
        "\n",
        "  #Using Function\n",
        "\n",
        "  model1 = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='linear', input_shape=(1,))\n",
        "  ])\n",
        "  model1.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "  lwi = Linear_weight_initializer(x, y)\n",
        "  weights = lwi.weights_calc()\n",
        "\n",
        "  model1.set_weights(weights)\n",
        "\n",
        "  history = model1.fit(x, y, epochs=10, verbose=0)\n",
        "  mse1, mae1 = model1.evaluate(x, y, verbose=0)\n",
        "\n",
        "  mse1, mae1 = \"{:.4f}\".format(mse1), \"{:.4f}\".format(mae1)\n",
        "\n",
        "  print(\"{:<7} {:<8} {:<25} {:<10} {:<10} {:<8} {:<25} {:<10} {:<10}\".format(str(int(noise/2+1))+'/50','MAE Normal:', mae0, 'MAE Function:', mae1, 'MSE Normal:', mse0, 'MSE Function:', mse1))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/50    MAE Normal: 3002146.7500              MAE Function: 0.1125     MSE Normal: 12017236705280.0000       MSE Function: 0.0416    \n",
            "2/50    MAE Normal: 15253059.0000             MAE Function: 1.6750     MSE Normal: 310209750761472.0000      MSE Function: 4.1549    \n",
            "3/50    MAE Normal: 2576213.0000              MAE Function: 3.3206     MSE Normal: 8849058693120.0000        MSE Function: 15.3489   \n",
            "4/50    MAE Normal: 21564596.0000             MAE Function: 2.3246     MSE Normal: 620050000117760.0000      MSE Function: 9.2400    \n",
            "5/50    MAE Normal: 11493297.0000             MAE Function: 2.1379     MSE Normal: 176127599443968.0000      MSE Function: 8.2808    \n",
            "6/50    MAE Normal: 9292400.0000              MAE Function: 2.7740     MSE Normal: 115132168404992.0000      MSE Function: 13.1860   \n",
            "7/50    MAE Normal: 19053598.0000             MAE Function: 3.6989     MSE Normal: 484064758857728.0000      MSE Function: 22.9591   \n",
            "8/50    MAE Normal: 21982770.0000             MAE Function: 3.7916     MSE Normal: 644334684733440.0000      MSE Function: 25.5737   \n",
            "9/50    MAE Normal: 12749150.0000             MAE Function: 4.1647     MSE Normal: 216724553072640.0000      MSE Function: 30.6475   \n",
            "10/50   MAE Normal: 674857.0625               MAE Function: 4.9210     MSE Normal: 606910283776.0000         MSE Function: 40.9393   \n",
            "11/50   MAE Normal: 18042284.0000             MAE Function: 5.4225     MSE Normal: 434034161221632.0000      MSE Function: 50.6061   \n",
            "12/50   MAE Normal: 11583927.0000             MAE Function: 5.9394     MSE Normal: 178919428849664.0000      MSE Function: 60.7087   \n",
            "13/50   MAE Normal: 20711048.0000             MAE Function: 7.3650     MSE Normal: 571938514665472.0000      MSE Function: 87.1296   \n",
            "14/50   MAE Normal: 6440153.5000              MAE Function: 7.4681     MSE Normal: 55302149898240.0000       MSE Function: 91.9942   \n",
            "15/50   MAE Normal: 5341077.0000              MAE Function: 8.3090     MSE Normal: 38034028363776.0000       MSE Function: 111.7125  \n",
            "16/50   MAE Normal: 2929538.0000              MAE Function: 8.0711     MSE Normal: 11442690457600.0000       MSE Function: 110.7564  \n",
            "17/50   MAE Normal: 9459490.0000              MAE Function: 8.2417     MSE Normal: 119312597647360.0000      MSE Function: 118.7556  \n",
            "18/50   MAE Normal: 1629924.5000              MAE Function: 8.7718     MSE Normal: 3542052503552.0000        MSE Function: 134.3774  \n",
            "19/50   MAE Normal: 14245393.0000             MAE Function: 9.4284     MSE Normal: 270575859662848.0000      MSE Function: 152.7741  \n",
            "20/50   MAE Normal: 5450921.5000              MAE Function: 9.7536     MSE Normal: 39616342130688.0000       MSE Function: 166.8665  \n",
            "21/50   MAE Normal: 4552892.0000              MAE Function: 10.3929    MSE Normal: 27637074362368.0000       MSE Function: 187.8025  \n",
            "22/50   MAE Normal: 20836464.0000             MAE Function: 12.0044    MSE Normal: 578885221613568.0000      MSE Function: 238.0159  \n",
            "23/50   MAE Normal: 5185482.0000              MAE Function: 11.3333    MSE Normal: 35851564220416.0000       MSE Function: 220.5792  \n",
            "24/50   MAE Normal: 14187513.0000             MAE Function: 15.1368    MSE Normal: 268380359622656.0000      MSE Function: 361.2122  \n",
            "25/50   MAE Normal: 22419714.0000             MAE Function: 14.0867    MSE Normal: 670203339866112.0000      MSE Function: 325.5563  \n",
            "26/50   MAE Normal: 1026705.7500              MAE Function: 12.7598    MSE Normal: 1405307977728.0000        MSE Function: 287.6461  \n",
            "27/50   MAE Normal: 18936696.0000             MAE Function: 15.9413    MSE Normal: 478136361811968.0000      MSE Function: 410.9778  \n",
            "28/50   MAE Normal: 19597382.0000             MAE Function: 13.9200    MSE Normal: 512086199238656.0000      MSE Function: 337.5565  \n",
            "29/50   MAE Normal: 15778606.0000             MAE Function: 14.2295    MSE Normal: 331959666475008.0000      MSE Function: 355.2558  \n",
            "30/50   MAE Normal: 15764357.0000             MAE Function: 15.1246    MSE Normal: 331354210304000.0000      MSE Function: 393.5578  \n",
            "31/50   MAE Normal: 8708732.0000              MAE Function: 15.4303    MSE Normal: 101125315362816.0000      MSE Function: 417.2895  \n",
            "32/50   MAE Normal: 21551824.0000             MAE Function: 17.4873    MSE Normal: 619317842411520.0000      MSE Function: 502.0646  \n",
            "33/50   MAE Normal: 10834467.0000             MAE Function: 17.3811    MSE Normal: 156518540378112.0000      MSE Function: 508.0245  \n",
            "34/50   MAE Normal: 10077273.0000             MAE Function: 17.5631    MSE Normal: 135401343811584.0000      MSE Function: 526.9575  \n",
            "35/50   MAE Normal: 4318549.5000              MAE Function: 19.3306    MSE Normal: 24864450674688.0000       MSE Function: 611.3488  \n",
            "36/50   MAE Normal: 3975078.5000              MAE Function: 19.0737    MSE Normal: 21068009963520.0000       MSE Function: 612.1340  \n",
            "37/50   MAE Normal: 8669161.0000              MAE Function: 18.3855    MSE Normal: 100203306680320.0000      MSE Function: 595.4572  \n",
            "38/50   MAE Normal: 12050850.0000             MAE Function: 20.1195    MSE Normal: 193629188521984.0000      MSE Function: 676.9680  \n",
            "39/50   MAE Normal: 10922251.0000             MAE Function: 19.2421    MSE Normal: 159060624146432.0000      MSE Function: 651.4094  \n",
            "40/50   MAE Normal: 12228956.0000             MAE Function: 20.6356    MSE Normal: 199396440408064.0000      MSE Function: 731.8123  \n",
            "41/50   MAE Normal: 22391150.0000             MAE Function: 20.5756    MSE Normal: 668492600705024.0000      MSE Function: 735.0120  \n",
            "42/50   MAE Normal: 3877304.7500              MAE Function: 20.6974    MSE Normal: 20044935331840.0000       MSE Function: 757.1558  \n",
            "43/50   MAE Normal: 1821089.0000              MAE Function: 22.3598    MSE Normal: 4420758667264.0000        MSE Function: 851.6945  \n",
            "44/50   MAE Normal: 3752124.0000              MAE Function: 22.1411    MSE Normal: 18771645300736.0000       MSE Function: 850.3542  \n",
            "45/50   MAE Normal: 17298988.0000             MAE Function: 22.7317    MSE Normal: 399009038467072.0000      MSE Function: 898.4001  \n",
            "46/50   MAE Normal: 15563303.0000             MAE Function: 23.2830    MSE Normal: 322955603083264.0000      MSE Function: 939.2183  \n",
            "47/50   MAE Normal: 5729094.0000              MAE Function: 23.8210    MSE Normal: 43764525563904.0000       MSE Function: 985.8813  \n",
            "48/50   MAE Normal: 21389054.0000             MAE Function: 24.2067    MSE Normal: 609995884331008.0000      MSE Function: 1021.8960 \n",
            "49/50   MAE Normal: 9064694.0000              MAE Function: 24.8621    MSE Normal: 109560236867584.0000      MSE Function: 1076.4919 \n",
            "50/50   MAE Normal: 17541382.0000             MAE Function: 26.8114    MSE Normal: 410276650287104.0000      MSE Function: 1204.0697 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emgz_lOkdHXi",
        "colab_type": "text"
      },
      "source": [
        "10 epochs way better results but totally useless since single feature datasets are extremely rare anyways and realistically impossible to find"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBDbVWPDd-gV",
        "colab_type": "text"
      },
      "source": [
        "Now to move on to perceptrons with a single feature. Logistic Regression. So we know that for Logistic y is given as a function of f(x) where alpha and beta exist in the form that y = 1/(1 + e^-f(x)). Now here, f(x) = alpha * x + beta. So, then what is the threshold thing. So generally anything greater than 0.5 and anything lesser than 0.5 are classified as a 1 and anything below is a 0. Therefore, let us think of a way to faster approach alpha and beta. First let's simply create a dataset and evaluate it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt5U2CZmaxjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array([10 * random.random() + random.random() * random.randint(-3, 3) for i in range(5000)] + [10 + 10 * random.random() + random.random() * random.randint(-3, 3) for i in range(5000)])\n",
        "y = np.array([0 for i in range(5000)] + [1 for i in range(5000)])\n",
        "\n",
        "indexes = np.array(range(10000))\n",
        "np.random.shuffle(indexes)\n",
        "x = x[indexes]\n",
        "y = y[indexes]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZZip2YGNJgc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94b54abc-a9ca-4910-fcc9-a37c0c0104fc"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x, y, epochs=200, verbose=1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.7159 - accuracy: 0.4769\n",
            "Epoch 2/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.5544 - accuracy: 0.6226\n",
            "Epoch 3/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.5022 - accuracy: 0.7169\n",
            "Epoch 4/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.7800\n",
            "Epoch 5/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.4250 - accuracy: 0.8183\n",
            "Epoch 6/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.3949 - accuracy: 0.8478\n",
            "Epoch 7/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.3688 - accuracy: 0.8681\n",
            "Epoch 8/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.3460 - accuracy: 0.8842\n",
            "Epoch 9/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.3260 - accuracy: 0.8986\n",
            "Epoch 10/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.3083 - accuracy: 0.9057\n",
            "Epoch 11/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.2926 - accuracy: 0.9153\n",
            "Epoch 12/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.2787 - accuracy: 0.9198\n",
            "Epoch 13/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.2662 - accuracy: 0.9262\n",
            "Epoch 14/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.2551 - accuracy: 0.9298\n",
            "Epoch 15/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.2450 - accuracy: 0.9336\n",
            "Epoch 16/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.2356 - accuracy: 0.9351\n",
            "Epoch 17/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.2272 - accuracy: 0.9375\n",
            "Epoch 18/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.2196 - accuracy: 0.9397\n",
            "Epoch 19/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.2126 - accuracy: 0.9419\n",
            "Epoch 20/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.2062 - accuracy: 0.9435\n",
            "Epoch 21/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.2005 - accuracy: 0.9444\n",
            "Epoch 22/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1951 - accuracy: 0.9456\n",
            "Epoch 23/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.9460\n",
            "Epoch 24/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.9466\n",
            "Epoch 25/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 0.9471\n",
            "Epoch 26/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.9473\n",
            "Epoch 27/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.9473\n",
            "Epoch 28/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1710 - accuracy: 0.9479\n",
            "Epoch 29/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9477\n",
            "Epoch 30/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.9477\n",
            "Epoch 31/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1628 - accuracy: 0.9482\n",
            "Epoch 32/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1605 - accuracy: 0.9486\n",
            "Epoch 33/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9480\n",
            "Epoch 34/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1562 - accuracy: 0.9488\n",
            "Epoch 35/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9486\n",
            "Epoch 36/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9497\n",
            "Epoch 37/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9483\n",
            "Epoch 38/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9499\n",
            "Epoch 39/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9487\n",
            "Epoch 40/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9504\n",
            "Epoch 41/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9493\n",
            "Epoch 42/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9492\n",
            "Epoch 43/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9492\n",
            "Epoch 44/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9494\n",
            "Epoch 45/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.9500\n",
            "Epoch 46/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9494\n",
            "Epoch 47/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.9504\n",
            "Epoch 48/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9505\n",
            "Epoch 49/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9507\n",
            "Epoch 50/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9502\n",
            "Epoch 51/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.9505\n",
            "Epoch 52/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1352 - accuracy: 0.9500\n",
            "Epoch 53/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9505\n",
            "Epoch 54/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9509\n",
            "Epoch 55/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.9505\n",
            "Epoch 56/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9502\n",
            "Epoch 57/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9503\n",
            "Epoch 58/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9509\n",
            "Epoch 59/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9508\n",
            "Epoch 60/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9505\n",
            "Epoch 61/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9509\n",
            "Epoch 62/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9507\n",
            "Epoch 63/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9507\n",
            "Epoch 64/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9498\n",
            "Epoch 65/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9510\n",
            "Epoch 66/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9505\n",
            "Epoch 67/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9508\n",
            "Epoch 68/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9511\n",
            "Epoch 69/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9512\n",
            "Epoch 70/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1266 - accuracy: 0.9502\n",
            "Epoch 71/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9516\n",
            "Epoch 72/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9511\n",
            "Epoch 73/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9508\n",
            "Epoch 74/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9512\n",
            "Epoch 75/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9515\n",
            "Epoch 76/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9510\n",
            "Epoch 77/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1244 - accuracy: 0.9510\n",
            "Epoch 78/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9504\n",
            "Epoch 79/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9509\n",
            "Epoch 80/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9508\n",
            "Epoch 81/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9512\n",
            "Epoch 82/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9516\n",
            "Epoch 83/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9516\n",
            "Epoch 84/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9508\n",
            "Epoch 85/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9507\n",
            "Epoch 86/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9508\n",
            "Epoch 87/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9512\n",
            "Epoch 88/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9513\n",
            "Epoch 89/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9510\n",
            "Epoch 90/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9509\n",
            "Epoch 91/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.9513\n",
            "Epoch 92/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9509\n",
            "Epoch 93/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9506\n",
            "Epoch 94/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9515\n",
            "Epoch 95/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9512\n",
            "Epoch 96/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9510\n",
            "Epoch 97/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.9508\n",
            "Epoch 98/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.9510\n",
            "Epoch 99/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9504\n",
            "Epoch 100/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1201 - accuracy: 0.9525\n",
            "Epoch 101/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9508\n",
            "Epoch 102/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9512\n",
            "Epoch 103/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9523\n",
            "Epoch 104/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9510\n",
            "Epoch 105/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9507\n",
            "Epoch 106/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9513\n",
            "Epoch 107/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9513\n",
            "Epoch 108/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9512\n",
            "Epoch 109/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9511\n",
            "Epoch 110/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9510\n",
            "Epoch 111/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9517\n",
            "Epoch 112/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9508\n",
            "Epoch 113/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9508\n",
            "Epoch 114/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9522\n",
            "Epoch 115/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9514\n",
            "Epoch 116/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9507\n",
            "Epoch 117/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9517\n",
            "Epoch 118/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.9510\n",
            "Epoch 119/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9501\n",
            "Epoch 120/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9514\n",
            "Epoch 121/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9511\n",
            "Epoch 122/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9512\n",
            "Epoch 123/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9513\n",
            "Epoch 124/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9513\n",
            "Epoch 125/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9511\n",
            "Epoch 126/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9514\n",
            "Epoch 127/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9510\n",
            "Epoch 128/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9517\n",
            "Epoch 129/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9511\n",
            "Epoch 130/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9516\n",
            "Epoch 131/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9515\n",
            "Epoch 132/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9519\n",
            "Epoch 133/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9522\n",
            "Epoch 134/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9510\n",
            "Epoch 135/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9520\n",
            "Epoch 136/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9514\n",
            "Epoch 137/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9517\n",
            "Epoch 138/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9519\n",
            "Epoch 139/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9514\n",
            "Epoch 140/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9513\n",
            "Epoch 141/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9518\n",
            "Epoch 142/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9516\n",
            "Epoch 143/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9520\n",
            "Epoch 144/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9514\n",
            "Epoch 145/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.9513\n",
            "Epoch 146/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.9511\n",
            "Epoch 147/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9518\n",
            "Epoch 148/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9517\n",
            "Epoch 149/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9512\n",
            "Epoch 150/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9517\n",
            "Epoch 151/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9512\n",
            "Epoch 152/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9512\n",
            "Epoch 153/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9513\n",
            "Epoch 154/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9513\n",
            "Epoch 155/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9519\n",
            "Epoch 156/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9516\n",
            "Epoch 157/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9521\n",
            "Epoch 158/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9518\n",
            "Epoch 159/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9514\n",
            "Epoch 160/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9513\n",
            "Epoch 161/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9512\n",
            "Epoch 162/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9515\n",
            "Epoch 163/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9517\n",
            "Epoch 164/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9513\n",
            "Epoch 165/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9511\n",
            "Epoch 166/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9518\n",
            "Epoch 167/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9522\n",
            "Epoch 168/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9511\n",
            "Epoch 169/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.9515\n",
            "Epoch 170/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9512\n",
            "Epoch 171/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9516\n",
            "Epoch 172/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.9511\n",
            "Epoch 173/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9509\n",
            "Epoch 174/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9513\n",
            "Epoch 175/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9519\n",
            "Epoch 176/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9522\n",
            "Epoch 177/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9522\n",
            "Epoch 178/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9515\n",
            "Epoch 179/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9517\n",
            "Epoch 180/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9515\n",
            "Epoch 181/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9511\n",
            "Epoch 182/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9513\n",
            "Epoch 183/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9516\n",
            "Epoch 184/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9515\n",
            "Epoch 185/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9517\n",
            "Epoch 186/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9518\n",
            "Epoch 187/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9515\n",
            "Epoch 188/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9519\n",
            "Epoch 189/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9514\n",
            "Epoch 190/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9517\n",
            "Epoch 191/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9522\n",
            "Epoch 192/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9513\n",
            "Epoch 193/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9514\n",
            "Epoch 194/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9519\n",
            "Epoch 195/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9521\n",
            "Epoch 196/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9516\n",
            "Epoch 197/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9519\n",
            "Epoch 198/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9522\n",
            "Epoch 199/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9522\n",
            "Epoch 200/200\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is7p4MlTVPCl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54e7d1c9-fb00-4532-c7e2-b04c26a80e26"
      },
      "source": [
        "print(model.get_weights())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[1.2285122]], dtype=float32), array([-12.144905], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3MfQ99sNuAY",
        "colab_type": "text"
      },
      "source": [
        "But, one thing to notice about logistic regression is that f(x) must be 0 at the line of seperation but then what's the use. Easiest way to pull that off is to have m = 1 and c = x0. So basically how to find x0. If we sort and then find the most confusing part of y to find out the most optimal x0, thats useless because then you are simply doing a sorting when np.sort is of the complexity ==> n log n and then to top off it we sample root(n) and all let's leave all of that we will go for the most simple method for sampling. So, if we sample out root(n) randomly from the shuffled dataset, and then we sort it we get, root(n) log(n)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4xdl_cpgSmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sigmoid_weight_initializer:\n",
        "  def __init__(self, x, y, sample_exp=0.5, eps=1e-5, max_iter=1000):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.x_sub = None\n",
        "    self.y_sub = None\n",
        "    self.sets = None\n",
        "    self.scores = None\n",
        "    self.c = None\n",
        "    self.weights = None\n",
        "    self.i = 0\n",
        "    self.eps = eps\n",
        "    self.max_iter = max_iter\n",
        "\n",
        "  def sub_generator(self):\n",
        "    self.x_sub = x[int(x.shape[0]**0.5)*self.i:int(x.shape[0]**0.5)*(self.i+1)]\n",
        "    self.y_sub = y[int(x.shape[0]**0.5)*self.i:int(x.shape[0]**0.5)*(self.i+1)]\n",
        "    indexes = np.argsort(self.x_sub)\n",
        "    self.x_sub = self.x_sub[indexes]\n",
        "    self.y_sub = self.y_sub[indexes]\n",
        "\n",
        "\n",
        "  def set_generator(self):\n",
        "    self.sub_generator()\n",
        "    set_size = int(self.x_sub.shape[0]**0.5)\n",
        "    self.sets = np.array([[self.x_sub[set_size*i:(i+1)*set_size], self.y_sub[set_size*i:(i+1)*set_size]] for i in range(self.y_sub.shape[0]//set_size)])\n",
        "\n",
        "  def set_scorer(self):\n",
        "    self.set_generator()\n",
        "    self.scores = np.abs(np.array([np.mean(set_i[1]) for set_i in self.sets]) - 0.5)\n",
        "    self.best_set = np.argsort(self.scores)[0]\n",
        "    \n",
        "  def weighter(self):\n",
        "    self.set_scorer()\n",
        "    self.c = - (np.median(self.sets[self.best_set][0]) + np.mean(self.sets[self.best_set][0]))/2\n",
        "    self.weights = [np.array([[1]]), np.array([self.c])]\n",
        "    return self.weights, self.c\n",
        "\n",
        "  def dissimilarity_matrix(self, data):\n",
        "    dissimilarity_matrix = np.empty([data.shape[0], data.shape[0]])\n",
        "    for i in range(len(data)):\n",
        "      for j in range(len(data)):\n",
        "        d_ij = np.sum(np.square(data[i]-data[j]))\n",
        "        dissimilarity_matrix[i][j] = d_ij\n",
        "\n",
        "    return dissimilarity_matrix\n",
        "\n",
        "  def get_perplexity(self, D_row, variance):\n",
        "    A_row = np.exp(-D_row * variance)\n",
        "    sumA = sum(A_row)\n",
        "    perplexity = np.log(sumA) + variance * np.sum(D_row * A_row) / sumA\n",
        "    return perplexity, A_row\n",
        "\n",
        "  def affinity_matrix(self, dMatrix, perplexity):\n",
        "    eps = self.eps\n",
        "    (n, _) = dMatrix.shape\n",
        "    variance_matrix = np.ones(dMatrix.shape[0])\n",
        "    affinity_matrix = np.zeros(dMatrix.shape)\n",
        "    logU = np.log(perplexity)\n",
        "    for i in range(dMatrix.shape[0]):\n",
        "      variance_min = -np.inf\n",
        "      variance_max =  np.inf\n",
        "      d_i = dMatrix[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]\n",
        "      (c_perplexity, thisA) = self.get_perplexity(d_i, variance_matrix[i])\n",
        "      perplexity_diff = c_perplexity - logU\n",
        "      tries = 0\n",
        "      while (np.isnan(perplexity_diff) or np.abs(perplexity_diff) > eps) and tries < self.max_iter:\n",
        "        if np.isnan(perplexity_diff):\n",
        "          variance_matrix[i] = variance_matrix[i] / 10.0\n",
        "        elif perplexity_diff > 0:\n",
        "          variance_min = variance_matrix[i].copy()\n",
        "          if variance_max == np.inf or variance_max == -np.inf:\n",
        "            variance_matrix[i] = variance_matrix[i] * 2.0\n",
        "          else:\n",
        "            variance_matrix[i] = (variance_matrix[i] + variance_max) / 2.0\n",
        "        else:\n",
        "          variance_max = variance_matrix[i].copy()\n",
        "          if variance_min == np.inf or variance_min == -np.inf:\n",
        "            variance_matrix[i] = variance_matrix[i] / 2.0\n",
        "          else:\n",
        "            variance_matrix[i] = (variance_matrix[i] + variance_min) / 2.0\n",
        "        (c_perplexity, thisA) = self.get_perplexity(d_i, variance_matrix[i])\n",
        "        perplexity_diff = c_perplexity - logU\n",
        "        tries += 1\n",
        "      affinity_matrix[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisA\n",
        "    return variance_matrix, affinity_matrix\n",
        "\n",
        "  def binding_matrix(self, aMatrix):\n",
        "    binding_matrix = aMatrix / aMatrix.sum(axis=1)[:,np.newaxis]\n",
        "    return binding_matrix\n",
        "\n",
        "  def outlier_probability(self, bMatrix):\n",
        "    outlier_matrix = np.prod(1-bMatrix, 0)\n",
        "    return outlier_matrix\n",
        "\n",
        "  def sos(self, cleaned_data, perplexity): \n",
        "    dMatrix = self.dissimilarity_matrix(cleaned_data)\n",
        "    var_matrix, aff_matrix = self.affinity_matrix(dMatrix, perplexity)\n",
        "    bin_matrix = self.binding_matrix(aff_matrix)\n",
        "    outlier_matrix = self.outlier_probability(bin_matrix)\n",
        "    return outlier_matrix\n",
        "\n",
        "  def weights_calc(self):\n",
        "    weights_list = []\n",
        "    for i in range(int(self.x.shape[0]/int(self.x.shape[0]**0.5))):\n",
        "      self.i = i\n",
        "      _, c = self.weighter()\n",
        "      weights_list.append(c)\n",
        "    weights_list = np.array(weights_list)\n",
        "    weights_list1 = (weights_list - np.min(weights_list))/(np.max(weights_list) - np.min(weights_list))\n",
        "    outlier_matrix = self.sos(weights_list1, weights_list1.shape[0]/4)\n",
        "    outlier_matrix_best = np.argmin(outlier_matrix)\n",
        "    self.c = weights_list[outlier_matrix_best]\n",
        "\n",
        "    self.weights = [np.array([[1]]), np.array([self.c])]\n",
        "    return self.weights"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srqkIvyga3Vs",
        "colab_type": "text"
      },
      "source": [
        "A poorer performance compared to the linear but one thing we can do is implent run and it is more dependent on the randomness so yes, it can wrongly classify a better implementation would be if we are able to make it so that the class samples root(n) multiple times, and then use outlier classifaction for removing these values and hence the function get_weights is implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHnQFWIYQTMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "swi = Sigmoid_weight_initializer(x, y)\n",
        "weights = swi.weights_calc()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq7eeL92TYRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "658ba26c-b858-4d69-dbe1-8bdd83660f55"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(1,))\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.set_weights(weights)\n",
        "history = model.fit(x, y, epochs=20, verbose=1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9512\n",
            "Epoch 2/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9518\n",
            "Epoch 3/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9512\n",
            "Epoch 4/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9517\n",
            "Epoch 5/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.9515\n",
            "Epoch 6/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.9513\n",
            "Epoch 7/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.9508\n",
            "Epoch 8/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9516\n",
            "Epoch 9/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9514\n",
            "Epoch 10/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9514\n",
            "Epoch 11/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9517\n",
            "Epoch 12/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9514\n",
            "Epoch 13/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9520\n",
            "Epoch 14/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9516\n",
            "Epoch 15/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9513\n",
            "Epoch 16/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9518\n",
            "Epoch 17/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9511\n",
            "Epoch 18/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9508\n",
            "Epoch 19/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9511\n",
            "Epoch 20/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DmIvmUdlaSB",
        "colab_type": "text"
      },
      "source": [
        "As you can see much similar accuracy and loss even though it is run for only 20 epochs and taking a look at the first epoch itself we can see a much closer start to the end result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pll9CA2AT99Y",
        "colab_type": "text"
      },
      "source": [
        "Well now, it starts much better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cuFORmjUZQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def noisy_sigmoid_dataset(noise):\n",
        "  x = np.array([10 * random.random() + random.random() * random.randint(-noise, noise) for i in range(5000)] + [100 + 10 * random.random() + random.random() * random.randint(-noise, noise) for i in range(5000)])\n",
        "  y = np.array([0 for i in range(5000)] + [1 for i in range(5000)])\n",
        "\n",
        "  indexes = np.array(range(10000))\n",
        "  np.random.shuffle(indexes)\n",
        "  x = x[indexes]\n",
        "  y = y[indexes]\n",
        "  return x, y"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6abTE2tmHO6",
        "colab_type": "text"
      },
      "source": [
        "The outputs which contain Function are the outputs from the class above and the ones with Normal are the ones which tensorflow does in the same number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgcE5dcRTzwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "9c60ea1d-37ef-478b-9530-7053fc7bf117"
      },
      "source": [
        "for noise in range(0, 100, 2):\n",
        "  x, y = noisy_sigmoid_dataset(noise)\n",
        "\n",
        "  #Normally\n",
        "\n",
        "  model0 = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(1,))\n",
        "  ])\n",
        "  model0.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  history = model0.fit(x, y, epochs=10, verbose=0)\n",
        "  loss0, acc0 = model0.evaluate(x, y, verbose=0)\n",
        "\n",
        "  loss0, acc0 = \"{:.4f}\".format(loss0), \"{:.4f}\".format(acc0)\n",
        "\n",
        "  #Using Function\n",
        "\n",
        "  model1 = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(1,))\n",
        "  ])\n",
        "  model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  lwi = Sigmoid_weight_initializer(x, y)\n",
        "  weights = lwi.weights_calc()\n",
        "\n",
        "  model1.set_weights(weights)\n",
        "\n",
        "  history = model1.fit(x, y, epochs=10, verbose=0)\n",
        "  loss1, acc1 = model1.evaluate(x, y, verbose=0)\n",
        "\n",
        "  loss1, acc1 = \"{:.4f}\".format(loss1), \"{:.4f}\".format(acc1)\n",
        "\n",
        "  print(\"{:<7} {:<8} {:<25} {:<10} {:<10} {:<8} {:<25} {:<10} {:<10}\".format(str(int(noise/2+1))+'/50','Acc Normal:', acc0, 'Acc Function:', acc1, 'Loss Normal:', loss0, 'Loss Function:', loss1))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/50    Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.2998                    Loss Function: 0.0000    \n",
            "2/50    Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.2999                    Loss Function: 0.0000    \n",
            "3/50    Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0823                    Loss Function: 0.0000    \n",
            "4/50    Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0938                    Loss Function: 0.0000    \n",
            "5/50    Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0771                    Loss Function: 0.0000    \n",
            "6/50    Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0778                    Loss Function: 0.0000    \n",
            "7/50    Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.1040                    Loss Function: 0.0000    \n",
            "8/50    Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.1174                    Loss Function: 0.0000    \n",
            "9/50    Acc Normal: 0.7153                    Acc Function: 1.0000     Loss Normal: 0.3735                    Loss Function: 0.0000    \n",
            "10/50   Acc Normal: 0.9615                    Acc Function: 1.0000     Loss Normal: 0.2743                    Loss Function: 0.0000    \n",
            "11/50   Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.1065                    Loss Function: 0.0000    \n",
            "12/50   Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0782                    Loss Function: 0.0000    \n",
            "13/50   Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0803                    Loss Function: 0.0000    \n",
            "14/50   Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0893                    Loss Function: 0.0000    \n",
            "15/50   Acc Normal: 0.9917                    Acc Function: 1.0000     Loss Normal: 0.1710                    Loss Function: 0.0000    \n",
            "16/50   Acc Normal: 0.6339                    Acc Function: 1.0000     Loss Normal: 0.4178                    Loss Function: 0.0000    \n",
            "17/50   Acc Normal: 0.9993                    Acc Function: 1.0000     Loss Normal: 0.0861                    Loss Function: 0.0000    \n",
            "18/50   Acc Normal: 0.9985                    Acc Function: 1.0000     Loss Normal: 0.0952                    Loss Function: 0.0001    \n",
            "19/50   Acc Normal: 0.9913                    Acc Function: 1.0000     Loss Normal: 0.1292                    Loss Function: 0.0000    \n",
            "20/50   Acc Normal: 0.9929                    Acc Function: 1.0000     Loss Normal: 0.0988                    Loss Function: 0.0000    \n",
            "21/50   Acc Normal: 0.9635                    Acc Function: 1.0000     Loss Normal: 0.1933                    Loss Function: 0.0000    \n",
            "22/50   Acc Normal: 0.9886                    Acc Function: 1.0000     Loss Normal: 0.1135                    Loss Function: 0.0000    \n",
            "23/50   Acc Normal: 0.9862                    Acc Function: 1.0000     Loss Normal: 0.1131                    Loss Function: 0.0001    \n",
            "24/50   Acc Normal: 0.9887                    Acc Function: 1.0000     Loss Normal: 0.1036                    Loss Function: 0.0001    \n",
            "25/50   Acc Normal: 0.9845                    Acc Function: 0.9999     Loss Normal: 0.1178                    Loss Function: 0.0004    \n",
            "26/50   Acc Normal: 0.9801                    Acc Function: 0.9994     Loss Normal: 0.1296                    Loss Function: 0.0016    \n",
            "27/50   Acc Normal: 0.9679                    Acc Function: 0.9990     Loss Normal: 0.1522                    Loss Function: 0.0022    \n",
            "28/50   Acc Normal: 0.9400                    Acc Function: 0.9966     Loss Normal: 0.2187                    Loss Function: 0.0115    \n",
            "29/50   Acc Normal: 0.9728                    Acc Function: 0.9947     Loss Normal: 0.1415                    Loss Function: 0.0214    \n",
            "30/50   Acc Normal: 0.8746                    Acc Function: 0.9932     Loss Normal: 0.3201                    Loss Function: 0.0206    \n",
            "31/50   Acc Normal: 0.9672                    Acc Function: 0.9912     Loss Normal: 0.1386                    Loss Function: 0.0287    \n",
            "32/50   Acc Normal: 0.9728                    Acc Function: 0.9903     Loss Normal: 0.1374                    Loss Function: 0.0348    \n",
            "33/50   Acc Normal: 0.6979                    Acc Function: 0.9847     Loss Normal: 0.4294                    Loss Function: 0.0740    \n",
            "34/50   Acc Normal: 0.6656                    Acc Function: 0.9823     Loss Normal: 0.4525                    Loss Function: 0.0975    \n",
            "35/50   Acc Normal: 0.9626                    Acc Function: 0.9793     Loss Normal: 0.1460                    Loss Function: 0.1019    \n",
            "36/50   Acc Normal: 0.9598                    Acc Function: 0.9767     Loss Normal: 0.1536                    Loss Function: 0.2157    \n",
            "37/50   Acc Normal: 0.9620                    Acc Function: 0.9733     Loss Normal: 0.1570                    Loss Function: 0.1750    \n",
            "38/50   Acc Normal: 0.7144                    Acc Function: 0.9646     Loss Normal: 0.4348                    Loss Function: 0.2093    \n",
            "39/50   Acc Normal: 0.9274                    Acc Function: 0.9666     Loss Normal: 0.2348                    Loss Function: 0.3083    \n",
            "40/50   Acc Normal: 0.9527                    Acc Function: 0.9645     Loss Normal: 0.1662                    Loss Function: 0.2478    \n",
            "41/50   Acc Normal: 0.9100                    Acc Function: 0.9585     Loss Normal: 0.2757                    Loss Function: 0.4749    \n",
            "42/50   Acc Normal: 0.9395                    Acc Function: 0.9537     Loss Normal: 0.1826                    Loss Function: 0.4018    \n",
            "43/50   Acc Normal: 0.9422                    Acc Function: 0.9539     Loss Normal: 0.1797                    Loss Function: 0.5496    \n",
            "44/50   Acc Normal: 0.9375                    Acc Function: 0.9462     Loss Normal: 0.1932                    Loss Function: 0.4885    \n",
            "45/50   Acc Normal: 0.9341                    Acc Function: 0.9437     Loss Normal: 0.1958                    Loss Function: 0.5924    \n",
            "46/50   Acc Normal: 0.9327                    Acc Function: 0.9383     Loss Normal: 0.1982                    Loss Function: 0.6301    \n",
            "47/50   Acc Normal: 0.9246                    Acc Function: 0.9382     Loss Normal: 0.2274                    Loss Function: 0.6856    \n",
            "48/50   Acc Normal: 0.8141                    Acc Function: 0.9300     Loss Normal: 0.3971                    Loss Function: 1.1321    \n",
            "49/50   Acc Normal: 0.8229                    Acc Function: 0.9300     Loss Normal: 0.3924                    Loss Function: 1.0910    \n",
            "50/50   Acc Normal: 0.7777                    Acc Function: 0.9232     Loss Normal: 0.4266                    Loss Function: 0.8671    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4se0nsSmi6a",
        "colab_type": "text"
      },
      "source": [
        "Yes, the loss kind of looses it for the function we built since it does not really take into account loss at all. This is the major drawback for Sigmoid Function so I guess accuracy is the only feature which is consitently better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2wW64PWX7gE",
        "colab_type": "text"
      },
      "source": [
        "So, finally now we have two functions one which optimally is able to intialize weights for Linear and one for Logistic. Hope you enojoyed reading."
      ]
    }
  ]
}