{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Discussing the Perception of Perceptrons.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EE3vev9mQpX",
        "colab_type": "text"
      },
      "source": [
        "# Introduction:\n",
        "\n",
        "#### What are Perceptrons:\n",
        "In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.\n",
        "\n",
        "Small, fast and compact these Perceptrons are able to learn and develop an understanding for so many Datasets. Yes, they may not be as efficient an accurate as Deep Neural Networks but they are still extremely useful models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWIj4g52nkYh",
        "colab_type": "text"
      },
      "source": [
        "Let us begin by understanding what perceptrons really are by creating and deploying on for a simple dataset where everything greater than 0.5 can be labelled as a 1 and everything below can be labelled as 0. \n",
        "\n",
        "Before beginning Perceptrons though let us look at an even simpler model called the \"Linear Regression Model\" I guess unless there is a name like Perceptron for it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo6tsygToOQP",
        "colab_type": "text"
      },
      "source": [
        "### Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-kPBQ94l-1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h5yV4s-oVG9",
        "colab_type": "text"
      },
      "source": [
        "### Creating A Dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C-2_C_NyEww",
        "colab_type": "text"
      },
      "source": [
        "Creating a Dataset around y = mx + c where both are defined below I will also create some noise and variation so that might not be the optimal answer finally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRgt1D3HoUeH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d463a328-73fd-4a79-812c-94f6f0590889"
      },
      "source": [
        "m = 121.7094\n",
        "c = 891.2648\n",
        "\n",
        "x = np.array([i for i in range(0, 50000, 3)][:16000])\n",
        "y = np.array([m*i+c+random.randint(-2, 2)*random.random() for i in range(0, 50000, 3)][:16000])\n",
        "print(y.shape, x.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16000,) (16000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYgTGf0FzZjL",
        "colab_type": "text"
      },
      "source": [
        "### Shuffling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scPLizzboTpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = np.array(range(y.shape[0]))\n",
        "np.random.shuffle(index)\n",
        "\n",
        "x = x[index]\n",
        "y = y[index]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCTUkI8T0Rjt",
        "colab_type": "text"
      },
      "source": [
        "### Model and Fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KisZFf3qzmmM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f0c06466-d1b6-4c40-82b4-340965cac2aa"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='linear')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "history = model.fit(x, y, epochs=200, verbose=0)\n",
        "print(model.evaluate(x, y))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 0s 876us/step - loss: 504089837568.0000 - mae: 615060.4375\n",
            "[504089837568.0, 615060.4375]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MfRAKJk7fR2",
        "colab_type": "text"
      },
      "source": [
        "Scaling it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRCAd0F27hhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_scalar = x[index]/np.max(x)\n",
        "y_scalar = y[index]/np.max(y)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8C-js5b1WX_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ae1a0271-e912-4cfd-dc7a-71b7159a16c5"
      },
      "source": [
        "model1 = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='linear')\n",
        "])\n",
        "model1.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "history = model1.fit(x_scalar, y_scalar, epochs=200, verbose=0)\n",
        "print(model.evaluate(x, y))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 0s 998us/step - loss: 504089837568.0000 - mae: 615060.4375\n",
            "[504089837568.0, 615060.4375]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG71e9RCDo-T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f1d4de9-ee20-4ede-b79d-8c135245c62b"
      },
      "source": [
        "y_pred = model.predict(x_scalar) * np.max(y)\n",
        "mae = np.mean(np.abs(y - y_pred))\n",
        "print(mae)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "850215580.4442718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGqIz2YBfbJm",
        "colab_type": "text"
      },
      "source": [
        "So basically scaling it down is somehow giving poor results so we won't scale anything down"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIAGCw3bJlsM",
        "colab_type": "text"
      },
      "source": [
        "Let us work it out now, Sampling and taking a 2 * root(n) sample use that to initialize the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ejayFIXI37M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = np.array(range(y.shape[0]))\n",
        "np.random.shuffle(index)\n",
        "\n",
        "x = x[index]\n",
        "y = y[index]\n",
        "\n",
        "set_1 = np.array([x[:int(x.shape[0]**0.5)], y[:int(x.shape[0]**0.5)]]).T\n",
        "set_2 = np.array([x[-int(x.shape[0]**0.5):], y[-int(x.shape[0]**0.5):]]).T"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itJgd5lUKjUT",
        "colab_type": "text"
      },
      "source": [
        "Now the equation of line is given by y = mx +c\n",
        "\n",
        "so m = (y1 - y2)/(x1 - x2)\n",
        "\n",
        "   c = y - mx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubRHYJgRJ3Oo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = np.array([(i[1]-j[1])/(i[0]-j[0]) for i,j in zip(set_1, set_2)])\n",
        "c = np.array([set_i[1] - m_i * set_i[0] for set_i, m_i in zip(set_1, m)])\n",
        "\n",
        "mae_anal_set_1 = np.array([sum([abs(set_i[1] - (set_i[0]*m_i + c_i)) for set_i in set_1])/set_1.shape[0] for m_i, c_i in zip(m, c)])\n",
        "\n",
        "sorted_index = np.argsort(mae_anal_set_1)\n",
        "m = m[sorted_index]\n",
        "c = c[sorted_index]\n",
        "mae_anal_set_1 = mae_anal_set_1[sorted_index]\n",
        "\n",
        "final_m, final_c = np.mean(m[:int(m.shape[0]**0.5)]), np.mean(c[:int(m.shape[0]**0.5)])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymkKb07wTeok",
        "colab_type": "text"
      },
      "source": [
        "Now Let us Initialize the weights accoridng to above method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHzo1JR9LA8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b1ecca4-07dd-4c2d-82d5-290b6b86f5ca"
      },
      "source": [
        "print(model.get_weights())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[96.113335]], dtype=float32), array([97.96404], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5TzvNUqSb5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = [np.array([[final_m]]), np.array([final_c])]\n",
        "model.set_weights(weights)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1GVjeNCT4W0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "67b1eb76-d32e-4fd4-b10d-54bd13472da1"
      },
      "source": [
        "history = model.fit(x, y, epochs=10, verbose=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 55468.5117 - mae: 202.8892\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 57176.1328 - mae: 207.0607\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 57176.1602 - mae: 207.0606\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 57176.1172 - mae: 207.0607\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 57176.1523 - mae: 207.0606\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 57176.1406 - mae: 207.0606\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 57176.1211 - mae: 207.0607\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 57176.1367 - mae: 207.0606\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 57176.1016 - mae: 207.0607\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 56670.1953 - mae: 206.1368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly1do-9ZVTG-",
        "colab_type": "text"
      },
      "source": [
        "Now Let us Create a Workable class for this where input is x and y. Output is weights which can directly be done as model.set_weights(weights)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5XxQQzrVBNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear_weight_initializer:\n",
        "  def __init__(self, x, y, sample_exp=0.5):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.sample_exp = sample_exp\n",
        "    self.set_1 = None\n",
        "    self.set_2 = None\n",
        "    self.final_c = 0\n",
        "    self.final_m = 0\n",
        "  \n",
        "  def setter(self):\n",
        "    index = np.array(range(y.shape[0]))\n",
        "    np.random.shuffle(index)\n",
        "\n",
        "    self.x = self.x[index]\n",
        "    self.y = self.y[index]\n",
        "\n",
        "    self.set_1 = np.array([self.x[:int(self.x.shape[0]**self.sample_exp)], self.y[:int(self.x.shape[0]**self.sample_exp)]]).T\n",
        "    self.set_2 = np.array([self.x[-int(self.x.shape[0]**self.sample_exp):], self.y[-int(self.x.shape[0]**self.sample_exp):]]).T\n",
        "\n",
        "  def m_c_calculator(self):\n",
        "    m = np.array([(i[1]-j[1])/(i[0]-j[0]) for i,j in zip(self.set_1, self.set_2)])\n",
        "    c = np.array([set_i[1] - m_i * set_i[0] for set_i, m_i in zip(self.set_1, m)])\n",
        "\n",
        "    mae_anal_set_1 = np.array([sum([abs(set_i[1] - (set_i[0]*m_i + c_i)) for set_i in self.set_1])/self.set_1.shape[0] for m_i, c_i in zip(m, c)])\n",
        "\n",
        "    sorted_index = np.argsort(mae_anal_set_1)\n",
        "    m = m[sorted_index]\n",
        "    c = c[sorted_index]\n",
        "    mae_anal_set_1 = mae_anal_set_1[sorted_index]\n",
        "\n",
        "    self.final_m, self.final_c = np.mean(m[:int(m.shape[0]**self.sample_exp)]), np.mean(c[:int(m.shape[0]**self.sample_exp)])\n",
        "\n",
        "  def weights_calc(self):\n",
        "    self.setter()\n",
        "    self.m_c_calculator()\n",
        "    weights = [np.array([[self.final_m]]), np.array([self.final_c])]\n",
        "    return weights"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b2y_aHRXz1n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f590ba1-3aa8-4bd0-fd2d-a3b6991f6bdf"
      },
      "source": [
        "lwi = Linear_weight_initializer(x, y)\n",
        "weights = lwi.weights_calc()\n",
        "print(weights)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[121.70939787]]), array([891.31568849])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epvTeF5MX_8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "190eb6a7-5480-4eed-994e-89e2a5c61bb0"
      },
      "source": [
        "model.set_weights(weights)\n",
        "history = model.fit(x, y, epochs=10, verbose=1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.7015 - mae: 0.6230\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.7015 - mae: 0.6230\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.7015 - mae: 0.6230\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.7015 - mae: 0.6230\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.7015 - mae: 0.6230\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 0s 915us/step - loss: 0.7015 - mae: 0.6230\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 0s 908us/step - loss: 0.7015 - mae: 0.6230\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 0s 921us/step - loss: 0.7015 - mae: 0.6230\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 0s 918us/step - loss: 0.7015 - mae: 0.6230\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 0s 926us/step - loss: 0.7015 - mae: 0.6230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSPMZFdVYLke",
        "colab_type": "text"
      },
      "source": [
        "Now Let us create a new Dataset with varying degrees of noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCQMZSGmYJdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def noisy_linear_dataset(n):\n",
        "  m = random.random()*1000\n",
        "  c = random.random()*1000\n",
        "\n",
        "  x = np.array([i for i in range(0, 50000, 3)][:16000])\n",
        "  y = np.array([m*i+c+random.randint(-n, n)*random.random() for i in range(0, 50000, 3)][:16000])\n",
        "\n",
        "  return x, y"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY_VkaZOY1ZR",
        "colab_type": "text"
      },
      "source": [
        "We will now run the model for 10 epochs one where tensorflow starts from the beginning and another where it is pre-optimised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEytIzC9mVPs",
        "colab_type": "text"
      },
      "source": [
        "The outputs which contain Function are the outputs from the class above and the ones with Normal are the ones which tensorflow does in the same number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5_f_eFaY0j5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "e4b88989-7b8c-4c09-a4ae-b1d177ca57d3"
      },
      "source": [
        "for noise in range(0, 100, 2):\n",
        "  x, y = noisy_linear_dataset(noise)\n",
        "\n",
        "  #Normally\n",
        "\n",
        "  model0 = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='linear', input_shape=(1,))\n",
        "  ])\n",
        "  model0.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "  history = model0.fit(x, y, epochs=10, verbose=0)\n",
        "  mse0, mae0 = model0.evaluate(x, y, verbose=0)\n",
        "\n",
        "  mse0, mae0 = \"{:.4f}\".format(mse0), \"{:.4f}\".format(mae0)\n",
        "\n",
        "  #Using Function\n",
        "\n",
        "  model1 = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='linear', input_shape=(1,))\n",
        "  ])\n",
        "  model1.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "  lwi = Linear_weight_initializer(x, y)\n",
        "  weights = lwi.weights_calc()\n",
        "\n",
        "  model1.set_weights(weights)\n",
        "\n",
        "  history = model1.fit(x, y, epochs=10, verbose=0)\n",
        "  mse1, mae1 = model1.evaluate(x, y, verbose=0)\n",
        "\n",
        "  mse1, mae1 = \"{:.4f}\".format(mse1), \"{:.4f}\".format(mae1)\n",
        "\n",
        "  print(\"{:<7} {:<8} {:<25} {:<10} {:<10} {:<8} {:<25} {:<10} {:<10}\".format(str(int(noise/2+1))+'/50','MAE Normal:', mae0, 'MAE Function:', mae1, 'MSE Normal:', mse0, 'MSE Function:', mse1))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/50    MAE Normal: 5872077.0000              MAE Function: 0.1879     MSE Normal: 45972860174336.0000       MSE Function: 0.1376    \n",
            "2/50    MAE Normal: 19854670.0000             MAE Function: 1.3253     MSE Normal: 525618164793344.0000      MSE Function: 3.3781    \n",
            "3/50    MAE Normal: 5072431.0000              MAE Function: 2.7752     MSE Normal: 34306424569856.0000       MSE Function: 11.0488   \n",
            "4/50    MAE Normal: 10406175.0000             MAE Function: 1.6333     MSE Normal: 144383613599744.0000      MSE Function: 4.9103    \n",
            "5/50    MAE Normal: 15192724.0000             MAE Function: 5.2947     MSE Normal: 307766518349824.0000      MSE Function: 40.2653   \n",
            "6/50    MAE Normal: 17302830.0000             MAE Function: 2.6631     MSE Normal: 399187682263040.0000      MSE Function: 12.9269   \n",
            "7/50    MAE Normal: 17940998.0000             MAE Function: 4.5541     MSE Normal: 429174607052800.0000      MSE Function: 32.6124   \n",
            "8/50    MAE Normal: 7026075.0000              MAE Function: 3.6211     MSE Normal: 65822085087232.0000       MSE Function: 23.3048   \n",
            "9/50    MAE Normal: 15217581.0000             MAE Function: 4.6202     MSE Normal: 308771742023680.0000      MSE Function: 35.4102   \n",
            "10/50   MAE Normal: 16368857.0000             MAE Function: 4.7286     MSE Normal: 357256788967424.0000      MSE Function: 39.1525   \n",
            "11/50   MAE Normal: 6055738.0000              MAE Function: 5.1591     MSE Normal: 48893920280576.0000       MSE Function: 46.9209   \n",
            "12/50   MAE Normal: 2355774.7500              MAE Function: 5.7060     MSE Normal: 7399302234112.0000        MSE Function: 57.0177   \n",
            "13/50   MAE Normal: 20692742.0000             MAE Function: 6.3123     MSE Normal: 570933089665024.0000      MSE Function: 69.6804   \n",
            "14/50   MAE Normal: 9475948.0000              MAE Function: 6.8292     MSE Normal: 119727078768640.0000      MSE Function: 81.3470   \n",
            "15/50   MAE Normal: 6340147.5000              MAE Function: 8.3044     MSE Normal: 53597899325440.0000       MSE Function: 111.4318  \n",
            "16/50   MAE Normal: 15331985.0000             MAE Function: 7.6848     MSE Normal: 313435673853952.0000      MSE Function: 104.6610  \n",
            "17/50   MAE Normal: 8669153.0000              MAE Function: 8.3978     MSE Normal: 100203650613248.0000      MSE Function: 121.3197  \n",
            "18/50   MAE Normal: 3930513.0000              MAE Function: 8.9287     MSE Normal: 20598963044352.0000       MSE Function: 136.2879  \n",
            "19/50   MAE Normal: 9959382.0000              MAE Function: 10.6093    MSE Normal: 132250431651840.0000      MSE Function: 182.9956  \n",
            "20/50   MAE Normal: 904690.4375               MAE Function: 9.8703     MSE Normal: 1090774433792.0000        MSE Function: 167.2627  \n",
            "21/50   MAE Normal: 6602237.5000              MAE Function: 10.6767    MSE Normal: 58120676048896.0000       MSE Function: 193.9291  \n",
            "22/50   MAE Normal: 21618046.0000             MAE Function: 11.2308    MSE Normal: 623136940752896.0000      MSE Function: 216.0859  \n",
            "23/50   MAE Normal: 18462182.0000             MAE Function: 12.2065    MSE Normal: 454480319479808.0000      MSE Function: 250.3223  \n",
            "24/50   MAE Normal: 14711229.0000             MAE Function: 13.3470    MSE Normal: 288559860809728.0000      MSE Function: 290.6552  \n",
            "25/50   MAE Normal: 10975302.0000             MAE Function: 12.2085    MSE Normal: 160613355487232.0000      MSE Function: 262.8838  \n",
            "26/50   MAE Normal: 13259209.0000             MAE Function: 12.9857    MSE Normal: 234415775023104.0000      MSE Function: 292.0869  \n",
            "27/50   MAE Normal: 2450746.0000              MAE Function: 13.1827    MSE Normal: 8006868140032.0000        MSE Function: 306.0766  \n",
            "28/50   MAE Normal: 12465546.0000             MAE Function: 13.8408    MSE Normal: 207188198948864.0000      MSE Function: 336.2102  \n",
            "29/50   MAE Normal: 11888905.0000             MAE Function: 14.9585    MSE Normal: 188461067796480.0000      MSE Function: 378.4191  \n",
            "30/50   MAE Normal: 1161989.8750              MAE Function: 18.5926    MSE Normal: 1799973371904.0000        MSE Function: 543.7738  \n",
            "31/50   MAE Normal: 13321748.0000             MAE Function: 15.7567    MSE Normal: 236625200152576.0000      MSE Function: 428.1945  \n",
            "32/50   MAE Normal: 14587561.0000             MAE Function: 16.6362    MSE Normal: 283737216516096.0000      MSE Function: 474.1109  \n",
            "33/50   MAE Normal: 20569296.0000             MAE Function: 18.5026    MSE Normal: 564138048749568.0000      MSE Function: 554.1912  \n",
            "34/50   MAE Normal: 7050409.0000              MAE Function: 17.1587    MSE Normal: 66276697309184.0000       MSE Function: 511.2812  \n",
            "35/50   MAE Normal: 7410107.5000              MAE Function: 19.6462    MSE Normal: 73210989117440.0000       MSE Function: 631.2186  \n",
            "36/50   MAE Normal: 12401759.0000             MAE Function: 18.0104    MSE Normal: 205074034851840.0000      MSE Function: 565.7376  \n",
            "37/50   MAE Normal: 16274250.0000             MAE Function: 19.4586    MSE Normal: 353139156844544.0000      MSE Function: 640.9639  \n",
            "38/50   MAE Normal: 1176813.2500              MAE Function: 19.0405    MSE Normal: 1846396452864.0000        MSE Function: 628.5256  \n",
            "39/50   MAE Normal: 2672253.7500              MAE Function: 19.1053    MSE Normal: 9520464003072.0000        MSE Function: 649.4064  \n",
            "40/50   MAE Normal: 17956130.0000             MAE Function: 20.2621    MSE Normal: 429905758126080.0000      MSE Function: 708.0381  \n",
            "41/50   MAE Normal: 16251398.0000             MAE Function: 20.6838    MSE Normal: 352147992477696.0000      MSE Function: 744.2357  \n",
            "42/50   MAE Normal: 17476542.0000             MAE Function: 21.1982    MSE Normal: 407243530960896.0000      MSE Function: 779.7261  \n",
            "43/50   MAE Normal: 10026706.0000             MAE Function: 21.7017    MSE Normal: 134048060014592.0000      MSE Function: 820.1337  \n",
            "44/50   MAE Normal: 3296503.0000              MAE Function: 22.4867    MSE Normal: 14489668812800.0000       MSE Function: 876.3323  \n",
            "45/50   MAE Normal: 15733393.0000             MAE Function: 22.9956    MSE Normal: 330058807902208.0000      MSE Function: 907.4713  \n",
            "46/50   MAE Normal: 2843961.5000              MAE Function: 23.3919    MSE Normal: 10782891835392.0000       MSE Function: 947.4765  \n",
            "47/50   MAE Normal: 7646754.0000              MAE Function: 24.1754    MSE Normal: 77965366919168.0000       MSE Function: 999.3434  \n",
            "48/50   MAE Normal: 11151721.0000             MAE Function: 27.6231    MSE Normal: 165813235482624.0000      MSE Function: 1244.7794 \n",
            "49/50   MAE Normal: 23610346.0000             MAE Function: 24.2917    MSE Normal: 743280933339136.0000      MSE Function: 1030.1422 \n",
            "50/50   MAE Normal: 5052034.5000              MAE Function: 28.3234    MSE Normal: 34028499501056.0000       MSE Function: 1301.9888 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emgz_lOkdHXi",
        "colab_type": "text"
      },
      "source": [
        "10 epochs way better results but totally useless since single feature datasets are extremely rare anyways and realistically impossible to find"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBDbVWPDd-gV",
        "colab_type": "text"
      },
      "source": [
        "Now to move on to perceptrons with a single feature. Logistic Regression. So we know that for Logistic y is given as a function of f(x) where alpha and beta exist in the form that y = 1/(1 + e^-f(x)). Now here, f(x) = alpha * x + beta. So, then what is the threshold thing. So generally anything greater than 0.5 and anything lesser than 0.5 are classified as a 1 and anything below is a 0. Therefore, let us think of a way to faster approach alpha and beta. First let's simply create a dataset and evaluate it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt5U2CZmaxjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array([10 * random.random() + random.random() * random.randint(-3, 3) for i in range(5000)] + [10 + 10 * random.random() + random.random() * random.randint(-3, 3) for i in range(5000)])\n",
        "y = np.array([0 for i in range(5000)] + [1 for i in range(5000)])\n",
        "\n",
        "indexes = np.array(range(10000))\n",
        "np.random.shuffle(indexes)\n",
        "x = x[indexes]\n",
        "y = y[indexes]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZZip2YGNJgc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "43377bb6-55ff-44c0-ec82-089166fdbd6f"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x, y, epochs=200, verbose=0)\n",
        "\n",
        "print(model.evaluate(x, y))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 998us/step - loss: 0.1006 - accuracy: 0.9610\n",
            "[0.10060581564903259, 0.9610000252723694]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is7p4MlTVPCl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53626f2d-1a6f-4394-8882-ae47460140bb"
      },
      "source": [
        "print(model.get_weights())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[1.2968458]], dtype=float32), array([-13.006274], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3MfQ99sNuAY",
        "colab_type": "text"
      },
      "source": [
        "But, one thing to notice about logistic regression is that f(x) must be 0 at the line of seperation but then what's the use. Easiest way to pull that off is to have m = 1 and c = x0. So basically how to find x0. If we sort and then find the most confusing part of y to find out the most optimal x0, thats useless because then you are simply doing a sorting when np.sort is of the complexity ==> n log n and then to top off it we sample root(n) and all let's leave all of that we will go for the most simple method for sampling. So, if we sample out root(n) randomly from the shuffled dataset, and then we sort it we get, root(n) log(n)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4xdl_cpgSmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sigmoid_weight_initializer:\n",
        "  def __init__(self, x, y, sample_exp=0.5, eps=1e-5, max_iter=1000):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.x_sub = None\n",
        "    self.y_sub = None\n",
        "    self.sets = None\n",
        "    self.scores = None\n",
        "    self.c = None\n",
        "    self.weights = None\n",
        "    self.i = 0\n",
        "    self.eps = eps\n",
        "    self.max_iter = max_iter\n",
        "\n",
        "  def sub_generator(self):\n",
        "    self.x_sub = x[int(x.shape[0]**0.5)*self.i:int(x.shape[0]**0.5)*(self.i+1)]\n",
        "    self.y_sub = y[int(x.shape[0]**0.5)*self.i:int(x.shape[0]**0.5)*(self.i+1)]\n",
        "    indexes = np.argsort(self.x_sub)\n",
        "    self.x_sub = self.x_sub[indexes]\n",
        "    self.y_sub = self.y_sub[indexes]\n",
        "\n",
        "\n",
        "  def set_generator(self):\n",
        "    self.sub_generator()\n",
        "    set_size = int(self.x_sub.shape[0]**0.5)\n",
        "    self.sets = np.array([[self.x_sub[set_size*i:(i+1)*set_size], self.y_sub[set_size*i:(i+1)*set_size]] for i in range(self.y_sub.shape[0]//set_size)])\n",
        "\n",
        "  def set_scorer(self):\n",
        "    self.set_generator()\n",
        "    self.scores = np.abs(np.array([np.mean(set_i[1]) for set_i in self.sets]) - 0.5)\n",
        "    self.best_set = np.argsort(self.scores)[0]\n",
        "    \n",
        "  def weighter(self):\n",
        "    self.set_scorer()\n",
        "    self.c = - (np.median(self.sets[self.best_set][0]) + np.mean(self.sets[self.best_set][0]))/2\n",
        "    self.weights = [np.array([[1]]), np.array([self.c])]\n",
        "    return self.weights, self.c\n",
        "\n",
        "  def dissimilarity_matrix(self, data):\n",
        "    dissimilarity_matrix = np.empty([data.shape[0], data.shape[0]])\n",
        "    for i in range(len(data)):\n",
        "      for j in range(len(data)):\n",
        "        d_ij = np.sum(np.square(data[i]-data[j]))\n",
        "        dissimilarity_matrix[i][j] = d_ij\n",
        "\n",
        "    return dissimilarity_matrix\n",
        "\n",
        "  def get_perplexity(self, D_row, variance):\n",
        "    A_row = np.exp(-D_row * variance)\n",
        "    sumA = sum(A_row)\n",
        "    perplexity = np.log(sumA) + variance * np.sum(D_row * A_row) / sumA\n",
        "    return perplexity, A_row\n",
        "\n",
        "  def affinity_matrix(self, dMatrix, perplexity):\n",
        "    eps = self.eps\n",
        "    (n, _) = dMatrix.shape\n",
        "    variance_matrix = np.ones(dMatrix.shape[0])\n",
        "    affinity_matrix = np.zeros(dMatrix.shape)\n",
        "    logU = np.log(perplexity)\n",
        "    for i in range(dMatrix.shape[0]):\n",
        "      variance_min = -np.inf\n",
        "      variance_max =  np.inf\n",
        "      d_i = dMatrix[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]\n",
        "      (c_perplexity, thisA) = self.get_perplexity(d_i, variance_matrix[i])\n",
        "      perplexity_diff = c_perplexity - logU\n",
        "      tries = 0\n",
        "      while (np.isnan(perplexity_diff) or np.abs(perplexity_diff) > eps) and tries < self.max_iter:\n",
        "        if np.isnan(perplexity_diff):\n",
        "          variance_matrix[i] = variance_matrix[i] / 10.0\n",
        "        elif perplexity_diff > 0:\n",
        "          variance_min = variance_matrix[i].copy()\n",
        "          if variance_max == np.inf or variance_max == -np.inf:\n",
        "            variance_matrix[i] = variance_matrix[i] * 2.0\n",
        "          else:\n",
        "            variance_matrix[i] = (variance_matrix[i] + variance_max) / 2.0\n",
        "        else:\n",
        "          variance_max = variance_matrix[i].copy()\n",
        "          if variance_min == np.inf or variance_min == -np.inf:\n",
        "            variance_matrix[i] = variance_matrix[i] / 2.0\n",
        "          else:\n",
        "            variance_matrix[i] = (variance_matrix[i] + variance_min) / 2.0\n",
        "        (c_perplexity, thisA) = self.get_perplexity(d_i, variance_matrix[i])\n",
        "        perplexity_diff = c_perplexity - logU\n",
        "        tries += 1\n",
        "      affinity_matrix[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisA\n",
        "    return variance_matrix, affinity_matrix\n",
        "\n",
        "  def binding_matrix(self, aMatrix):\n",
        "    binding_matrix = aMatrix / aMatrix.sum(axis=1)[:,np.newaxis]\n",
        "    return binding_matrix\n",
        "\n",
        "  def outlier_probability(self, bMatrix):\n",
        "    outlier_matrix = np.prod(1-bMatrix, 0)\n",
        "    return outlier_matrix\n",
        "\n",
        "  def sos(self, cleaned_data, perplexity): \n",
        "    dMatrix = self.dissimilarity_matrix(cleaned_data)\n",
        "    var_matrix, aff_matrix = self.affinity_matrix(dMatrix, perplexity)\n",
        "    bin_matrix = self.binding_matrix(aff_matrix)\n",
        "    outlier_matrix = self.outlier_probability(bin_matrix)\n",
        "    return outlier_matrix\n",
        "\n",
        "  def weights_calc(self):\n",
        "    weights_list = []\n",
        "    for i in range(int(self.x.shape[0]/int(self.x.shape[0]**0.5))):\n",
        "      self.i = i\n",
        "      _, c = self.weighter()\n",
        "      weights_list.append(c)\n",
        "    weights_list = np.array(weights_list)\n",
        "    weights_list1 = (weights_list - np.min(weights_list))/(np.max(weights_list) - np.min(weights_list))\n",
        "    outlier_matrix = self.sos(weights_list1, weights_list1.shape[0]/4)\n",
        "    outlier_matrix_best = np.argmin(outlier_matrix)\n",
        "    self.c = weights_list[outlier_matrix_best]\n",
        "\n",
        "    self.weights = [np.array([[1]]), np.array([self.c])]\n",
        "    return self.weights"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srqkIvyga3Vs",
        "colab_type": "text"
      },
      "source": [
        "A poorer performance compared to the linear but one thing we can do is implent run and it is more dependent on the randomness so yes, it can wrongly classify a better implementation would be if we are able to make it so that the class samples root(n) multiple times, and then use outlier classifaction for removing these values and hence the function get_weights is implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHnQFWIYQTMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "swi = Sigmoid_weight_initializer(x, y)\n",
        "weights = swi.weights_calc()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq7eeL92TYRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "c7e70583-50ea-41dd-c60a-e60f9e4b2319"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(1,))\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.set_weights(weights)\n",
        "history = model.fit(x, y, epochs=20, verbose=1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1140 - accuracy: 0.9582\n",
            "Epoch 2/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9606\n",
            "Epoch 3/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.9592\n",
            "Epoch 4/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9600\n",
            "Epoch 5/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9600\n",
            "Epoch 6/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9602\n",
            "Epoch 7/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9595\n",
            "Epoch 8/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9601\n",
            "Epoch 9/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9595\n",
            "Epoch 10/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9601\n",
            "Epoch 11/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9601\n",
            "Epoch 12/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9597\n",
            "Epoch 13/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9617\n",
            "Epoch 14/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9599\n",
            "Epoch 15/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9599\n",
            "Epoch 16/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9603\n",
            "Epoch 17/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9605\n",
            "Epoch 18/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9601\n",
            "Epoch 19/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9604\n",
            "Epoch 20/20\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DmIvmUdlaSB",
        "colab_type": "text"
      },
      "source": [
        "As you can see much similar accuracy and loss even though it is run for only 20 epochs and taking a look at the first epoch itself we can see a much closer start to the end result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pll9CA2AT99Y",
        "colab_type": "text"
      },
      "source": [
        "Well now, it starts much better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cuFORmjUZQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def noisy_sigmoid_dataset(noise):\n",
        "  x = np.array([10 * random.random() + random.random() * random.randint(-noise, noise) for i in range(5000)] + [100 + 10 * random.random() + random.random() * random.randint(-noise, noise) for i in range(5000)])\n",
        "  y = np.array([0 for i in range(5000)] + [1 for i in range(5000)])\n",
        "\n",
        "  indexes = np.array(range(10000))\n",
        "  np.random.shuffle(indexes)\n",
        "  x = x[indexes]\n",
        "  y = y[indexes]\n",
        "  return x, y"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6abTE2tmHO6",
        "colab_type": "text"
      },
      "source": [
        "The outputs which contain Function are the outputs from the class above and the ones with Normal are the ones which tensorflow does in the same number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgcE5dcRTzwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "10cc4560-84cc-42b8-9b76-dbf2e0f75dc6"
      },
      "source": [
        "for noise in range(0, 100, 2):\n",
        "  x, y = noisy_sigmoid_dataset(noise)\n",
        "\n",
        "  #Normally\n",
        "\n",
        "  model0 = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(1,))\n",
        "  ])\n",
        "  model0.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  history = model0.fit(x, y, epochs=10, verbose=0)\n",
        "  loss0, acc0 = model0.evaluate(x, y, verbose=0)\n",
        "\n",
        "  loss0, acc0 = \"{:.4f}\".format(loss0), \"{:.4f}\".format(acc0)\n",
        "\n",
        "  #Using Function\n",
        "\n",
        "  model1 = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(1,))\n",
        "  ])\n",
        "  model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  lwi = Sigmoid_weight_initializer(x, y)\n",
        "  weights = lwi.weights_calc()\n",
        "\n",
        "  model1.set_weights(weights)\n",
        "\n",
        "  history = model1.fit(x, y, epochs=10, verbose=0)\n",
        "  loss1, acc1 = model1.evaluate(x, y, verbose=0)\n",
        "\n",
        "  loss1, acc1 = \"{:.4f}\".format(loss1), \"{:.4f}\".format(acc1)\n",
        "\n",
        "  print(\"{:<7} {:<8} {:<25} {:<10} {:<10} {:<8} {:<25} {:<10} {:<10}\".format(str(int(noise/2+1))+'/50','Acc Normal:', acc0, 'Acc Function:', acc1, 'Loss Normal:', loss0, 'Loss Function:', loss1))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/50    Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0948                    Loss Function: 0.0000    \n",
            "2/50    Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0888                    Loss Function: 0.0000    \n",
            "3/50    Acc Normal: 0.5036                    Acc Function: 1.0000     Loss Normal: 0.4222                    Loss Function: 0.0000    \n",
            "4/50    Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0785                    Loss Function: 0.0000    \n",
            "5/50    Acc Normal: 0.9986                    Acc Function: 1.0000     Loss Normal: 0.2646                    Loss Function: 0.0000    \n",
            "6/50    Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0918                    Loss Function: 0.0000    \n",
            "7/50    Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0845                    Loss Function: 0.0000    \n",
            "8/50    Acc Normal: 0.6230                    Acc Function: 1.0000     Loss Normal: 0.3977                    Loss Function: 0.0000    \n",
            "9/50    Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0789                    Loss Function: 0.0000    \n",
            "10/50   Acc Normal: 0.9886                    Acc Function: 1.0000     Loss Normal: 0.2351                    Loss Function: 0.0000    \n",
            "11/50   Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0947                    Loss Function: 0.0000    \n",
            "12/50   Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0787                    Loss Function: 0.0001    \n",
            "13/50   Acc Normal: 1.0000                    Acc Function: 1.0000     Loss Normal: 0.0926                    Loss Function: 0.0000    \n",
            "14/50   Acc Normal: 0.9999                    Acc Function: 1.0000     Loss Normal: 0.0978                    Loss Function: 0.0000    \n",
            "15/50   Acc Normal: 0.9994                    Acc Function: 1.0000     Loss Normal: 0.0965                    Loss Function: 0.0000    \n",
            "16/50   Acc Normal: 0.9994                    Acc Function: 1.0000     Loss Normal: 0.0973                    Loss Function: 0.0000    \n",
            "17/50   Acc Normal: 0.9973                    Acc Function: 1.0000     Loss Normal: 0.1011                    Loss Function: 0.0000    \n",
            "18/50   Acc Normal: 0.6899                    Acc Function: 1.0000     Loss Normal: 0.3983                    Loss Function: 0.0000    \n",
            "19/50   Acc Normal: 0.8112                    Acc Function: 1.0000     Loss Normal: 0.3510                    Loss Function: 0.0001    \n",
            "20/50   Acc Normal: 0.9965                    Acc Function: 1.0000     Loss Normal: 0.0999                    Loss Function: 0.0000    \n",
            "21/50   Acc Normal: 0.9253                    Acc Function: 1.0000     Loss Normal: 0.2641                    Loss Function: 0.0001    \n",
            "22/50   Acc Normal: 0.9269                    Acc Function: 1.0000     Loss Normal: 0.2618                    Loss Function: 0.0001    \n",
            "23/50   Acc Normal: 0.8869                    Acc Function: 1.0000     Loss Normal: 0.3028                    Loss Function: 0.0001    \n",
            "24/50   Acc Normal: 0.9872                    Acc Function: 0.9999     Loss Normal: 0.1094                    Loss Function: 0.0002    \n",
            "25/50   Acc Normal: 0.9805                    Acc Function: 0.9999     Loss Normal: 0.1202                    Loss Function: 0.0006    \n",
            "26/50   Acc Normal: 0.9823                    Acc Function: 0.9996     Loss Normal: 0.1189                    Loss Function: 0.0017    \n",
            "27/50   Acc Normal: 0.9535                    Acc Function: 0.9991     Loss Normal: 0.2012                    Loss Function: 0.0026    \n",
            "28/50   Acc Normal: 0.9830                    Acc Function: 0.9978     Loss Normal: 0.1201                    Loss Function: 0.0050    \n",
            "29/50   Acc Normal: 0.7894                    Acc Function: 0.9945     Loss Normal: 0.3769                    Loss Function: 0.0209    \n",
            "30/50   Acc Normal: 0.9746                    Acc Function: 0.9934     Loss Normal: 0.1266                    Loss Function: 0.0216    \n",
            "31/50   Acc Normal: 0.9688                    Acc Function: 0.9903     Loss Normal: 0.1373                    Loss Function: 0.0460    \n",
            "32/50   Acc Normal: 0.9607                    Acc Function: 0.9891     Loss Normal: 0.1613                    Loss Function: 0.0444    \n",
            "33/50   Acc Normal: 0.9662                    Acc Function: 0.9854     Loss Normal: 0.1447                    Loss Function: 0.0586    \n",
            "34/50   Acc Normal: 0.6864                    Acc Function: 0.9815     Loss Normal: 0.4377                    Loss Function: 0.0894    \n",
            "35/50   Acc Normal: 0.9630                    Acc Function: 0.9783     Loss Normal: 0.1451                    Loss Function: 0.1278    \n",
            "36/50   Acc Normal: 0.8841                    Acc Function: 0.9758     Loss Normal: 0.3078                    Loss Function: 0.1453    \n",
            "37/50   Acc Normal: 0.9668                    Acc Function: 0.9722     Loss Normal: 0.1479                    Loss Function: 0.1811    \n",
            "38/50   Acc Normal: 0.9425                    Acc Function: 0.9664     Loss Normal: 0.1929                    Loss Function: 0.3115    \n",
            "39/50   Acc Normal: 0.9464                    Acc Function: 0.9634     Loss Normal: 0.1862                    Loss Function: 0.3516    \n",
            "40/50   Acc Normal: 0.9401                    Acc Function: 0.9601     Loss Normal: 0.2068                    Loss Function: 0.4141    \n",
            "41/50   Acc Normal: 0.9368                    Acc Function: 0.9572     Loss Normal: 0.1830                    Loss Function: 0.3421    \n",
            "42/50   Acc Normal: 0.9417                    Acc Function: 0.9555     Loss Normal: 0.1803                    Loss Function: 0.3937    \n",
            "43/50   Acc Normal: 0.9416                    Acc Function: 0.9497     Loss Normal: 0.1861                    Loss Function: 0.6249    \n",
            "44/50   Acc Normal: 0.9228                    Acc Function: 0.9443     Loss Normal: 0.2353                    Loss Function: 0.5698    \n",
            "45/50   Acc Normal: 0.7261                    Acc Function: 0.9428     Loss Normal: 0.4396                    Loss Function: 0.5408    \n",
            "46/50   Acc Normal: 0.9153                    Acc Function: 0.9399     Loss Normal: 0.2451                    Loss Function: 0.5586    \n",
            "47/50   Acc Normal: 0.9267                    Acc Function: 0.9378     Loss Normal: 0.2082                    Loss Function: 0.9366    \n",
            "48/50   Acc Normal: 0.9249                    Acc Function: 0.9286     Loss Normal: 0.2146                    Loss Function: 1.0915    \n",
            "49/50   Acc Normal: 0.8027                    Acc Function: 0.9263     Loss Normal: 0.4072                    Loss Function: 0.7308    \n",
            "50/50   Acc Normal: 0.8979                    Acc Function: 0.9209     Loss Normal: 0.2828                    Loss Function: 0.9617    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4se0nsSmi6a",
        "colab_type": "text"
      },
      "source": [
        "Yes, the loss kind of looses it for the function we built since it does not really take into account loss at all. This is the major drawback for Sigmoid Function so I guess accuracy is the only feature which is consitently better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2wW64PWX7gE",
        "colab_type": "text"
      },
      "source": [
        "So, finally now we have two functions one which optimally is able to intialize weights for Linear and one for Logistic. Hope you enojoyed reading."
      ]
    }
  ]
}